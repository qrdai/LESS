/var/spool/slurmd/job11812779/slurm_script: line 0: unalias: f95: not found
/var/spool/slurmd/job11812779/slurm_script: line 0: unalias: cc: not found
[2024-07-01 01:26:05,650] torch.distributed.run: [WARNING] master_addr is only used for static rdzv_backend and when rdzv_endpoint is not specified.
07/01/2024 01:26:38 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1distributed training: True, 16-bits training: False
07/01/2024 01:26:38 - INFO - __main__ - Training parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
analysis_dataset=bbh,
analysis_mode=False,
auto_find_batch_size=False,
bf16=True,
bf16_full_eval=False,
data_seed=3,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=False,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=IntervalStrategy.NO,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[<FSDPOption.FULL_SHARD: 'full_shard'>, <FSDPOption.AUTO_WRAP: 'auto_wrap'>],
fsdp_config={'fsdp_transformer_layer_cls_to_wrap': ['LlamaDecoderLayer'], 'fsdp_backward_prefetch': 'backward_pre', 'limit_all_gathers': 'true', 'use_orig_params': 'true', 'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=128,
gradient_checkpointing=False,
gradient_checkpointing_kwargs=None,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=HubStrategy.EVERY_SAVE,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=2e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=../out/Llama-2-7b-p0.05-lora-seed3-bsz128/runs/Jul01_01-26-34_ccc0422.campuscluster.illinois.edu,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=1.0,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_kwargs={},
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
neftune_noise_alpha=None,
no_cuda=False,
num_train_epochs=4.0,
optim=OptimizerNames.ADAMW_TORCH,
optim_args=None,
output_dir=../out/Llama-2-7b-p0.05-lora-seed3-bsz128,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=1,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['wandb'],
resume_from_checkpoint=None,
run_name=../out/Llama-2-7b-p0.05-lora-seed3-bsz128,
save_on_each_node=False,
save_only_model=False,
save_safetensors=True,
save_steps=500,
save_strategy=IntervalStrategy.EPOCH,
save_total_limit=None,
seed=0,
skip_memory_metrics=True,
split_batches=False,
tf32=False,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
train_dataset_names=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_mps_device=False,
warmup_ratio=0.03,
warmup_steps=0,
weight_decay=0.0,
)
07/01/2024 01:26:38 - INFO - __main__ - Model parameters ModelArguments(model_name_or_path='meta-llama/Llama-2-7b-hf', config_name=None, tokenizer_name=None, cache_dir=None, use_fast_tokenizer=True, model_revision='main', use_auth_token=False, torch_dtype=None, lora=True, lora_r=128, lora_alpha=512.0, lora_dropout=0.1, lora_target_modules=['q_proj', 'k_proj', 'v_proj', 'o_proj'])
07/01/2024 01:26:38 - INFO - __main__ - Dataset parameters DataArguments(train_files=['../data/train/processed/flan_v2/flan_v2_data.jsonl', '../data/train/processed/cot/cot_data.jsonl', '../data/train/processed/dolly/dolly_data.jsonl', '../data/train/processed/oasst1/oasst1_data.jsonl'], overwrite_cache=False, preprocessing_num_workers=None, max_seq_length=2048, sample_data_seed=42, percentage=0.05)
/projects/illinois/eng/cs/haopeng/qirundai/anaconda3/envs/less/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[INFO|tokenization_utils_base.py:2026] 2024-07-01 01:26:38,691 >> loading file tokenizer.model from cache at /projects/illinois/eng/cs/haopeng/qirundai/.cache/huggingface/transformers/models--meta-llama--Llama-2-7b-hf/snapshots/01c7f73d771dfac7d292323805ebc428287df4f9/tokenizer.model
[INFO|tokenization_utils_base.py:2026] 2024-07-01 01:26:38,691 >> loading file tokenizer.json from cache at /projects/illinois/eng/cs/haopeng/qirundai/.cache/huggingface/transformers/models--meta-llama--Llama-2-7b-hf/snapshots/01c7f73d771dfac7d292323805ebc428287df4f9/tokenizer.json
[INFO|tokenization_utils_base.py:2026] 2024-07-01 01:26:38,691 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2026] 2024-07-01 01:26:38,691 >> loading file special_tokens_map.json from cache at /projects/illinois/eng/cs/haopeng/qirundai/.cache/huggingface/transformers/models--meta-llama--Llama-2-7b-hf/snapshots/01c7f73d771dfac7d292323805ebc428287df4f9/special_tokens_map.json
[INFO|tokenization_utils_base.py:2026] 2024-07-01 01:26:38,691 >> loading file tokenizer_config.json from cache at /projects/illinois/eng/cs/haopeng/qirundai/.cache/huggingface/transformers/models--meta-llama--Llama-2-7b-hf/snapshots/01c7f73d771dfac7d292323805ebc428287df4f9/tokenizer_config.json
Using custom data configuration default-ae972da9ed8d9314
07/01/2024 01:26:38 - INFO - datasets.builder - Using custom data configuration default-ae972da9ed8d9314
Loading Dataset Infos from /projects/illinois/eng/cs/haopeng/qirundai/anaconda3/envs/less/lib/python3.11/site-packages/datasets/packaged_modules/json
07/01/2024 01:26:38 - INFO - datasets.info - Loading Dataset Infos from /projects/illinois/eng/cs/haopeng/qirundai/anaconda3/envs/less/lib/python3.11/site-packages/datasets/packaged_modules/json
Generating dataset json (/projects/illinois/eng/cs/haopeng/qirundai/.cache/huggingface/datasets/json/default-ae972da9ed8d9314/0.0.0/7483f22a71512872c377524b97484f6d20c275799bb9e7cd8fb3198178d8220a)
07/01/2024 01:26:39 - INFO - datasets.builder - Generating dataset json (/projects/illinois/eng/cs/haopeng/qirundai/.cache/huggingface/datasets/json/default-ae972da9ed8d9314/0.0.0/7483f22a71512872c377524b97484f6d20c275799bb9e7cd8fb3198178d8220a)
Downloading and preparing dataset json/default to /projects/illinois/eng/cs/haopeng/qirundai/.cache/huggingface/datasets/json/default-ae972da9ed8d9314/0.0.0/7483f22a71512872c377524b97484f6d20c275799bb9e7cd8fb3198178d8220a...
07/01/2024 01:26:39 - INFO - datasets.builder - Downloading and preparing dataset json/default to /projects/illinois/eng/cs/haopeng/qirundai/.cache/huggingface/datasets/json/default-ae972da9ed8d9314/0.0.0/7483f22a71512872c377524b97484f6d20c275799bb9e7cd8fb3198178d8220a...
Downloading took 0.0 min
07/01/2024 01:26:39 - INFO - datasets.download.download_manager - Downloading took 0.0 min
Checksum Computation took 0.0 min
07/01/2024 01:26:39 - INFO - datasets.download.download_manager - Checksum Computation took 0.0 min
Generating train split
07/01/2024 01:26:41 - INFO - datasets.builder - Generating train split
Generating train split: 0 examples [00:00, ? examples/s]Generating train split: 11589 examples [00:00, 67487.01 examples/s]Generating train split: 51999 examples [00:00, 196097.19 examples/s]Generating train split: 89543 examples [00:00, 162681.36 examples/s]Generating train split: 119182 examples [00:01, 85722.04 examples/s]Generating train split: 157458 examples [00:01, 107352.50 examples/s]Generating train split: 178694 examples [00:01, 120850.23 examples/s]Generating train split: 211139 examples [00:01, 133198.33 examples/s]Generating train split: 241803 examples [00:02, 115303.42 examples/s]Generating train split: 268851 examples [00:02, 131320.15 examples/s]Generating train split: 270679 examples [00:02, 122851.34 examples/s]
Unable to verify splits sizes.
07/01/2024 01:26:43 - INFO - datasets.utils.info_utils - Unable to verify splits sizes.
Dataset json downloaded and prepared to /projects/illinois/eng/cs/haopeng/qirundai/.cache/huggingface/datasets/json/default-ae972da9ed8d9314/0.0.0/7483f22a71512872c377524b97484f6d20c275799bb9e7cd8fb3198178d8220a. Subsequent calls will reuse this data.
07/01/2024 01:26:43 - INFO - datasets.builder - Dataset json downloaded and prepared to /projects/illinois/eng/cs/haopeng/qirundai/.cache/huggingface/datasets/json/default-ae972da9ed8d9314/0.0.0/7483f22a71512872c377524b97484f6d20c275799bb9e7cd8fb3198178d8220a. Subsequent calls will reuse this data.
Process #0 will write at /projects/illinois/eng/cs/haopeng/qirundai/.cache/huggingface/datasets/json/default-ae972da9ed8d9314/0.0.0/7483f22a71512872c377524b97484f6d20c275799bb9e7cd8fb3198178d8220a/cache-f112e7ed588f8144_00000_of_00010.arrow
07/01/2024 01:26:44 - INFO - datasets.arrow_dataset - Process #0 will write at /projects/illinois/eng/cs/haopeng/qirundai/.cache/huggingface/datasets/json/default-ae972da9ed8d9314/0.0.0/7483f22a71512872c377524b97484f6d20c275799bb9e7cd8fb3198178d8220a/cache-f112e7ed588f8144_00000_of_00010.arrow
Process #1 will write at /projects/illinois/eng/cs/haopeng/qirundai/.cache/huggingface/datasets/json/default-ae972da9ed8d9314/0.0.0/7483f22a71512872c377524b97484f6d20c275799bb9e7cd8fb3198178d8220a/cache-f112e7ed588f8144_00001_of_00010.arrow
07/01/2024 01:26:44 - INFO - datasets.arrow_dataset - Process #1 will write at /projects/illinois/eng/cs/haopeng/qirundai/.cache/huggingface/datasets/json/default-ae972da9ed8d9314/0.0.0/7483f22a71512872c377524b97484f6d20c275799bb9e7cd8fb3198178d8220a/cache-f112e7ed588f8144_00001_of_00010.arrow
Process #2 will write at /projects/illinois/eng/cs/haopeng/qirundai/.cache/huggingface/datasets/json/default-ae972da9ed8d9314/0.0.0/7483f22a71512872c377524b97484f6d20c275799bb9e7cd8fb3198178d8220a/cache-f112e7ed588f8144_00002_of_00010.arrow
07/01/2024 01:26:44 - INFO - datasets.arrow_dataset - Process #2 will write at /projects/illinois/eng/cs/haopeng/qirundai/.cache/huggingface/datasets/json/default-ae972da9ed8d9314/0.0.0/7483f22a71512872c377524b97484f6d20c275799bb9e7cd8fb3198178d8220a/cache-f112e7ed588f8144_00002_of_00010.arrow
Process #3 will write at /projects/illinois/eng/cs/haopeng/qirundai/.cache/huggingface/datasets/json/default-ae972da9ed8d9314/0.0.0/7483f22a71512872c377524b97484f6d20c275799bb9e7cd8fb3198178d8220a/cache-f112e7ed588f8144_00003_of_00010.arrow
07/01/2024 01:26:44 - INFO - datasets.arrow_dataset - Process #3 will write at /projects/illinois/eng/cs/haopeng/qirundai/.cache/huggingface/datasets/json/default-ae972da9ed8d9314/0.0.0/7483f22a71512872c377524b97484f6d20c275799bb9e7cd8fb3198178d8220a/cache-f112e7ed588f8144_00003_of_00010.arrow
Process #4 will write at /projects/illinois/eng/cs/haopeng/qirundai/.cache/huggingface/datasets/json/default-ae972da9ed8d9314/0.0.0/7483f22a71512872c377524b97484f6d20c275799bb9e7cd8fb3198178d8220a/cache-f112e7ed588f8144_00004_of_00010.arrow
07/01/2024 01:26:44 - INFO - datasets.arrow_dataset - Process #4 will write at /projects/illinois/eng/cs/haopeng/qirundai/.cache/huggingface/datasets/json/default-ae972da9ed8d9314/0.0.0/7483f22a71512872c377524b97484f6d20c275799bb9e7cd8fb3198178d8220a/cache-f112e7ed588f8144_00004_of_00010.arrow
Process #5 will write at /projects/illinois/eng/cs/haopeng/qirundai/.cache/huggingface/datasets/json/default-ae972da9ed8d9314/0.0.0/7483f22a71512872c377524b97484f6d20c275799bb9e7cd8fb3198178d8220a/cache-f112e7ed588f8144_00005_of_00010.arrow
07/01/2024 01:26:44 - INFO - datasets.arrow_dataset - Process #5 will write at /projects/illinois/eng/cs/haopeng/qirundai/.cache/huggingface/datasets/json/default-ae972da9ed8d9314/0.0.0/7483f22a71512872c377524b97484f6d20c275799bb9e7cd8fb3198178d8220a/cache-f112e7ed588f8144_00005_of_00010.arrow
Process #6 will write at /projects/illinois/eng/cs/haopeng/qirundai/.cache/huggingface/datasets/json/default-ae972da9ed8d9314/0.0.0/7483f22a71512872c377524b97484f6d20c275799bb9e7cd8fb3198178d8220a/cache-f112e7ed588f8144_00006_of_00010.arrow
07/01/2024 01:26:44 - INFO - datasets.arrow_dataset - Process #6 will write at /projects/illinois/eng/cs/haopeng/qirundai/.cache/huggingface/datasets/json/default-ae972da9ed8d9314/0.0.0/7483f22a71512872c377524b97484f6d20c275799bb9e7cd8fb3198178d8220a/cache-f112e7ed588f8144_00006_of_00010.arrow
Process #7 will write at /projects/illinois/eng/cs/haopeng/qirundai/.cache/huggingface/datasets/json/default-ae972da9ed8d9314/0.0.0/7483f22a71512872c377524b97484f6d20c275799bb9e7cd8fb3198178d8220a/cache-f112e7ed588f8144_00007_of_00010.arrow
07/01/2024 01:26:44 - INFO - datasets.arrow_dataset - Process #7 will write at /projects/illinois/eng/cs/haopeng/qirundai/.cache/huggingface/datasets/json/default-ae972da9ed8d9314/0.0.0/7483f22a71512872c377524b97484f6d20c275799bb9e7cd8fb3198178d8220a/cache-f112e7ed588f8144_00007_of_00010.arrow
Process #8 will write at /projects/illinois/eng/cs/haopeng/qirundai/.cache/huggingface/datasets/json/default-ae972da9ed8d9314/0.0.0/7483f22a71512872c377524b97484f6d20c275799bb9e7cd8fb3198178d8220a/cache-f112e7ed588f8144_00008_of_00010.arrow
07/01/2024 01:26:44 - INFO - datasets.arrow_dataset - Process #8 will write at /projects/illinois/eng/cs/haopeng/qirundai/.cache/huggingface/datasets/json/default-ae972da9ed8d9314/0.0.0/7483f22a71512872c377524b97484f6d20c275799bb9e7cd8fb3198178d8220a/cache-f112e7ed588f8144_00008_of_00010.arrow
Process #9 will write at /projects/illinois/eng/cs/haopeng/qirundai/.cache/huggingface/datasets/json/default-ae972da9ed8d9314/0.0.0/7483f22a71512872c377524b97484f6d20c275799bb9e7cd8fb3198178d8220a/cache-f112e7ed588f8144_00009_of_00010.arrow
07/01/2024 01:26:44 - INFO - datasets.arrow_dataset - Process #9 will write at /projects/illinois/eng/cs/haopeng/qirundai/.cache/huggingface/datasets/json/default-ae972da9ed8d9314/0.0.0/7483f22a71512872c377524b97484f6d20c275799bb9e7cd8fb3198178d8220a/cache-f112e7ed588f8144_00009_of_00010.arrow
Spawning 10 processes
07/01/2024 01:26:44 - INFO - datasets.arrow_dataset - Spawning 10 processes
Tokenizing and reformatting instruction data (num_proc=10):   0%|          | 0/13533 [00:00<?, ? examples/s]Caching processed dataset at /projects/illinois/eng/cs/haopeng/qirundai/.cache/huggingface/datasets/json/default-ae972da9ed8d9314/0.0.0/7483f22a71512872c377524b97484f6d20c275799bb9e7cd8fb3198178d8220a/cache-f112e7ed588f8144_00000_of_00010.arrow
07/01/2024 01:26:44 - INFO - datasets.arrow_dataset - Caching processed dataset at /projects/illinois/eng/cs/haopeng/qirundai/.cache/huggingface/datasets/json/default-ae972da9ed8d9314/0.0.0/7483f22a71512872c377524b97484f6d20c275799bb9e7cd8fb3198178d8220a/cache-f112e7ed588f8144_00000_of_00010.arrow
Caching processed dataset at /projects/illinois/eng/cs/haopeng/qirundai/.cache/huggingface/datasets/json/default-ae972da9ed8d9314/0.0.0/7483f22a71512872c377524b97484f6d20c275799bb9e7cd8fb3198178d8220a/cache-f112e7ed588f8144_00001_of_00010.arrow
07/01/2024 01:26:44 - INFO - datasets.arrow_dataset - Caching processed dataset at /projects/illinois/eng/cs/haopeng/qirundai/.cache/huggingface/datasets/json/default-ae972da9ed8d9314/0.0.0/7483f22a71512872c377524b97484f6d20c275799bb9e7cd8fb3198178d8220a/cache-f112e7ed588f8144_00001_of_00010.arrow
Caching processed dataset at /projects/illinois/eng/cs/haopeng/qirundai/.cache/huggingface/datasets/json/default-ae972da9ed8d9314/0.0.0/7483f22a71512872c377524b97484f6d20c275799bb9e7cd8fb3198178d8220a/cache-f112e7ed588f8144_00002_of_00010.arrow
07/01/2024 01:26:44 - INFO - datasets.arrow_dataset - Caching processed dataset at /projects/illinois/eng/cs/haopeng/qirundai/.cache/huggingface/datasets/json/default-ae972da9ed8d9314/0.0.0/7483f22a71512872c377524b97484f6d20c275799bb9e7cd8fb3198178d8220a/cache-f112e7ed588f8144_00002_of_00010.arrow
Caching processed dataset at /projects/illinois/eng/cs/haopeng/qirundai/.cache/huggingface/datasets/json/default-ae972da9ed8d9314/0.0.0/7483f22a71512872c377524b97484f6d20c275799bb9e7cd8fb3198178d8220a/cache-f112e7ed588f8144_00003_of_00010.arrow
07/01/2024 01:26:44 - INFO - datasets.arrow_dataset - Caching processed dataset at /projects/illinois/eng/cs/haopeng/qirundai/.cache/huggingface/datasets/json/default-ae972da9ed8d9314/0.0.0/7483f22a71512872c377524b97484f6d20c275799bb9e7cd8fb3198178d8220a/cache-f112e7ed588f8144_00003_of_00010.arrow
Tokenizing and reformatting instruction data (num_proc=10):   0%|          | 40/13533 [00:00<01:13, 183.92 examples/s]Caching processed dataset at /projects/illinois/eng/cs/haopeng/qirundai/.cache/huggingface/datasets/json/default-ae972da9ed8d9314/0.0.0/7483f22a71512872c377524b97484f6d20c275799bb9e7cd8fb3198178d8220a/cache-f112e7ed588f8144_00004_of_00010.arrow
07/01/2024 01:26:44 - INFO - datasets.arrow_dataset - Caching processed dataset at /projects/illinois/eng/cs/haopeng/qirundai/.cache/huggingface/datasets/json/default-ae972da9ed8d9314/0.0.0/7483f22a71512872c377524b97484f6d20c275799bb9e7cd8fb3198178d8220a/cache-f112e7ed588f8144_00004_of_00010.arrow
Caching processed dataset at /projects/illinois/eng/cs/haopeng/qirundai/.cache/huggingface/datasets/json/default-ae972da9ed8d9314/0.0.0/7483f22a71512872c377524b97484f6d20c275799bb9e7cd8fb3198178d8220a/cache-f112e7ed588f8144_00005_of_00010.arrow
07/01/2024 01:26:44 - INFO - datasets.arrow_dataset - Caching processed dataset at /projects/illinois/eng/cs/haopeng/qirundai/.cache/huggingface/datasets/json/default-ae972da9ed8d9314/0.0.0/7483f22a71512872c377524b97484f6d20c275799bb9e7cd8fb3198178d8220a/cache-f112e7ed588f8144_00005_of_00010.arrow
Caching processed dataset at /projects/illinois/eng/cs/haopeng/qirundai/.cache/huggingface/datasets/json/default-ae972da9ed8d9314/0.0.0/7483f22a71512872c377524b97484f6d20c275799bb9e7cd8fb3198178d8220a/cache-f112e7ed588f8144_00007_of_00010.arrow
07/01/2024 01:26:44 - INFO - datasets.arrow_dataset - Caching processed dataset at /projects/illinois/eng/cs/haopeng/qirundai/.cache/huggingface/datasets/json/default-ae972da9ed8d9314/0.0.0/7483f22a71512872c377524b97484f6d20c275799bb9e7cd8fb3198178d8220a/cache-f112e7ed588f8144_00007_of_00010.arrow
Caching processed dataset at /projects/illinois/eng/cs/haopeng/qirundai/.cache/huggingface/datasets/json/default-ae972da9ed8d9314/0.0.0/7483f22a71512872c377524b97484f6d20c275799bb9e7cd8fb3198178d8220a/cache-f112e7ed588f8144_00006_of_00010.arrow
07/01/2024 01:26:44 - INFO - datasets.arrow_dataset - Caching processed dataset at /projects/illinois/eng/cs/haopeng/qirundai/.cache/huggingface/datasets/json/default-ae972da9ed8d9314/0.0.0/7483f22a71512872c377524b97484f6d20c275799bb9e7cd8fb3198178d8220a/cache-f112e7ed588f8144_00006_of_00010.arrow
Tokenizing and reformatting instruction data (num_proc=10):   3%|▎         | 389/13533 [00:00<00:08, 1490.88 examples/s]Caching processed dataset at /projects/illinois/eng/cs/haopeng/qirundai/.cache/huggingface/datasets/json/default-ae972da9ed8d9314/0.0.0/7483f22a71512872c377524b97484f6d20c275799bb9e7cd8fb3198178d8220a/cache-f112e7ed588f8144_00009_of_00010.arrow
07/01/2024 01:26:44 - INFO - datasets.arrow_dataset - Caching processed dataset at /projects/illinois/eng/cs/haopeng/qirundai/.cache/huggingface/datasets/json/default-ae972da9ed8d9314/0.0.0/7483f22a71512872c377524b97484f6d20c275799bb9e7cd8fb3198178d8220a/cache-f112e7ed588f8144_00009_of_00010.arrow
Caching processed dataset at /projects/illinois/eng/cs/haopeng/qirundai/.cache/huggingface/datasets/json/default-ae972da9ed8d9314/0.0.0/7483f22a71512872c377524b97484f6d20c275799bb9e7cd8fb3198178d8220a/cache-f112e7ed588f8144_00008_of_00010.arrow
07/01/2024 01:26:44 - INFO - datasets.arrow_dataset - Caching processed dataset at /projects/illinois/eng/cs/haopeng/qirundai/.cache/huggingface/datasets/json/default-ae972da9ed8d9314/0.0.0/7483f22a71512872c377524b97484f6d20c275799bb9e7cd8fb3198178d8220a/cache-f112e7ed588f8144_00008_of_00010.arrow
Tokenizing and reformatting instruction data (num_proc=10):   8%|▊         | 1051/13533 [00:00<00:03, 3335.89 examples/s]Tokenizing and reformatting instruction data (num_proc=10):  13%|█▎        | 1777/13533 [00:00<00:02, 4604.07 examples/s]Tokenizing and reformatting instruction data (num_proc=10):  18%|█▊        | 2487/13533 [00:00<00:02, 5369.73 examples/s]Tokenizing and reformatting instruction data (num_proc=10):  23%|██▎       | 3162/13533 [00:00<00:01, 5798.71 examples/s]Tokenizing and reformatting instruction data (num_proc=10):  28%|██▊       | 3835/13533 [00:00<00:01, 6075.49 examples/s]Tokenizing and reformatting instruction data (num_proc=10):  34%|███▍      | 4579/13533 [00:00<00:01, 6411.60 examples/s]Tokenizing and reformatting instruction data (num_proc=10):  39%|███▉      | 5264/13533 [00:01<00:01, 6455.17 examples/s]Tokenizing and reformatting instruction data (num_proc=10):  44%|████▍     | 5938/13533 [00:01<00:01, 6497.16 examples/s]Tokenizing and reformatting instruction data (num_proc=10):  49%|████▉     | 6628/13533 [00:01<00:01, 6539.09 examples/s]Tokenizing and reformatting instruction data (num_proc=10):  54%|█████▍    | 7313/13533 [00:01<00:00, 6598.13 examples/s]Tokenizing and reformatting instruction data (num_proc=10):  59%|█████▉    | 7996/13533 [00:01<00:00, 6650.18 examples/s]Tokenizing and reformatting instruction data (num_proc=10):  64%|██████▍   | 8717/13533 [00:01<00:00, 6748.43 examples/s]Tokenizing and reformatting instruction data (num_proc=10):  70%|██████▉   | 9436/13533 [00:01<00:00, 6839.42 examples/s]Tokenizing and reformatting instruction data (num_proc=10):  75%|███████▍  | 10141/13533 [00:01<00:00, 5773.29 examples/s]Tokenizing and reformatting instruction data (num_proc=10):  79%|███████▉  | 10758/13533 [00:01<00:00, 5824.81 examples/s]Tokenizing and reformatting instruction data (num_proc=10):  84%|████████▍ | 11414/13533 [00:02<00:00, 5999.08 examples/s]Tokenizing and reformatting instruction data (num_proc=10):  90%|████████▉ | 12134/13533 [00:02<00:00, 6314.84 examples/s]Tokenizing and reformatting instruction data (num_proc=10):  94%|█████████▍| 12781/13533 [00:02<00:00, 6338.90 examples/s]Tokenizing and reformatting instruction data (num_proc=10):  99%|█████████▉| 13440/13533 [00:02<00:00, 5629.99 examples/s]Tokenizing and reformatting instruction data (num_proc=10): 100%|██████████| 13533/13533 [00:02<00:00, 5354.96 examples/s]
Concatenating 10 shards
07/01/2024 01:26:46 - INFO - datasets.arrow_dataset - Concatenating 10 shards
[INFO|configuration_utils.py:739] 2024-07-01 01:26:46,983 >> loading configuration file config.json from cache at /projects/illinois/eng/cs/haopeng/qirundai/.cache/huggingface/transformers/models--meta-llama--Llama-2-7b-hf/snapshots/01c7f73d771dfac7d292323805ebc428287df4f9/config.json
[INFO|configuration_utils.py:802] 2024-07-01 01:26:47,055 >> Model config LlamaConfig {
  "_name_or_path": "meta-llama/Llama-2-7b-hf",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_position_embeddings": 4096,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": null,
  "rope_theta": 10000.0,
  "tie_word_embeddings": false,
  "transformers_version": "4.36.2",
  "use_cache": true,
  "vocab_size": 32000
}

[INFO|modeling_utils.py:3344] 2024-07-01 01:26:49,981 >> loading weights file model.safetensors from cache at /projects/illinois/eng/cs/haopeng/qirundai/.cache/huggingface/transformers/models--meta-llama--Llama-2-7b-hf/snapshots/01c7f73d771dfac7d292323805ebc428287df4f9/model.safetensors.index.json
[INFO|configuration_utils.py:826] 2024-07-01 01:26:49,995 >> Generate config GenerationConfig {
  "bos_token_id": 1,
  "eos_token_id": 2
}

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:02<00:02,  2.80s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.59s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.77s/it]
[INFO|modeling_utils.py:4185] 2024-07-01 01:26:53,645 >> All model checkpoint weights were used when initializing LlamaForCausalLM.

[INFO|modeling_utils.py:4193] 2024-07-01 01:26:53,646 >> All the weights of LlamaForCausalLM were initialized from the model checkpoint at meta-llama/Llama-2-7b-hf.
If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.
[INFO|configuration_utils.py:781] 2024-07-01 01:26:53,692 >> loading configuration file generation_config.json from cache at /projects/illinois/eng/cs/haopeng/qirundai/.cache/huggingface/transformers/models--meta-llama--Llama-2-7b-hf/snapshots/01c7f73d771dfac7d292323805ebc428287df4f9/generation_config.json
[INFO|configuration_utils.py:826] 2024-07-01 01:26:53,692 >> Generate config GenerationConfig {
  "bos_token_id": 1,
  "do_sample": true,
  "eos_token_id": 2,
  "max_length": 4096,
  "pad_token_id": 0,
  "temperature": 0.6,
  "top_p": 0.9
}

[INFO|modeling_utils.py:1813] 2024-07-01 01:26:53,699 >> You are resizing the embedding layer without providing a `pad_to_multiple_of` parameter. This means that the new embedding dimension will be 32001. This might induce some performance reduction as *Tensor Cores* will not be available. For more details about this, or help on choosing the correct value for resizing, refer to this guide: https://docs.nvidia.com/deeplearning/performance/dl-performance-matrix-multiplication/index.html#requirements-tc
07/01/2024 01:26:58 - INFO - __main__ - Applied LoRA to model.
trainable params: 134,217,728 || all params: 6,872,641,536 || trainable%: 1.9529278123549145
Map:   0%|          | 0/13533 [00:00<?, ? examples/s]/projects/illinois/eng/cs/haopeng/qirundai/LESS/less/train/data_arguments.py:50: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  com_len = (torch.tensor(labels) > -1).sum()
Caching processed dataset at /projects/illinois/eng/cs/haopeng/qirundai/.cache/huggingface/datasets/json/default-ae972da9ed8d9314/0.0.0/7483f22a71512872c377524b97484f6d20c275799bb9e7cd8fb3198178d8220a/cache-c93004eb9c7c16d6.arrow
07/01/2024 01:26:59 - INFO - datasets.arrow_dataset - Caching processed dataset at /projects/illinois/eng/cs/haopeng/qirundai/.cache/huggingface/datasets/json/default-ae972da9ed8d9314/0.0.0/7483f22a71512872c377524b97484f6d20c275799bb9e7cd8fb3198178d8220a/cache-c93004eb9c7c16d6.arrow
Map:   7%|▋         | 1000/13533 [00:00<00:11, 1060.03 examples/s]Map:  22%|██▏       | 3000/13533 [00:01<00:03, 3195.41 examples/s]Map:  37%|███▋      | 5000/13533 [00:01<00:01, 5067.27 examples/s]Map:  52%|█████▏    | 7000/13533 [00:01<00:00, 6581.73 examples/s]Map:  67%|██████▋   | 9000/13533 [00:01<00:00, 7799.23 examples/s]Map:  81%|████████▏ | 11000/13533 [00:01<00:00, 8744.35 examples/s]Map:  96%|█████████▌| 13000/13533 [00:02<00:00, 8571.81 examples/s]Map: 100%|██████████| 13533/13533 [00:02<00:00, 6019.44 examples/s]
[train set] examples: 13533; # avg tokens: 370.9773254394531
[train set] examples: 13533; # avg completion tokens: 105.39820861816406
07/01/2024 01:27:00 - INFO - __main__ - Sample 6311 of the training set: {'input_ids': tensor([    1,   529, 29989,  1792, 29989, 29958,    13,  5618,   338, 13258,
          358,  9124,   292, 29973, 10604, 29901,    13, 29966, 29989,   465,
        22137, 29989, 29958,    13,   797, 10147,   358,  9124,   292,   639,
         2408, 29879,   304,  3058, 14188,   310,   263, 18161,  5786,  5001,
          470,   263, 17266,   403,  8542,   393,  5718,   297, 25228,   706,
        29899,  6707, 18161, 22160,   373,  2306,  3131,   310, 15724, 29892,
        17266,   800, 29892,   322,  4095,  1860, 29889, 18375, 17658,  6942,
          411, 17266,   403,  1436,   749, 29892,  1316,   263,  9124,  1795,
         6985,   297, 29263, 18161,  7483,   491,  1090, 16554,   470, 16684,
          408,   278,  3132, 29915, 29879, 10823,   297,   278, 17759,   749,
          310,  2553, 29873,   470,  1592,   537,   409,  2764,  1907, 29889,
          530, 13258,   358,  9124,  1122,   884,  6985, 14582,  9701,   297,
         2778,  5743,   322,  1274,  7680,  2187,   313, 29924, 29987, 29909,
        29897,   322,  3867, 10359,   453,   653,  5786,  1316,   408,  9999,
         3907, 29892,  3534,   292,   310, 25748,   322,  1592,   537,   409,
         2764,  1907, 29892,   383,  2965, 29907,  5786,   313, 20227, 17869,
        23643, 29892, 16256, 15942, 29892,   322,   844,   397,  1907, 29897,
          470,  5925,   313, 25254, 29872,  4599,   293, 29892, 16200,   470,
         1592,   537,  5925,   467,  7849, 13258,   358, 24388,  7344,  6019,
         2545,  3946,   482,   322, 24342, 10643,  5840,  1860,   297,  9589,
          651,   411,  1009, 13258,   358,  5925,  5381,   267, 29889,  1094,
          385, 13661, 29892,   372,   338,  9391,   701,   964,   278,  8313,
          479,  5032,  3522,   313, 21064, 26485,   511, 14253, 28794,   313,
         6563, 29899,  5563,  5381,   267,   511,   322, 25927,  1387,  9999,
          313, 18732,  1891,  5381,   267,   467,    13,    13,  2525,  4561,
        12128, 24388,   322,  3240,   737, 24388, 29892, 13258,   358, 24388,
          437,   451,  2125, 19754,  1169, 29889,     2, 29871,    13]), 'labels': tensor([ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,   797, 10147,   358,  9124,   292,   639,
         2408, 29879,   304,  3058, 14188,   310,   263, 18161,  5786,  5001,
          470,   263, 17266,   403,  8542,   393,  5718,   297, 25228,   706,
        29899,  6707, 18161, 22160,   373,  2306,  3131,   310, 15724, 29892,
        17266,   800, 29892,   322,  4095,  1860, 29889, 18375, 17658,  6942,
          411, 17266,   403,  1436,   749, 29892,  1316,   263,  9124,  1795,
         6985,   297, 29263, 18161,  7483,   491,  1090, 16554,   470, 16684,
          408,   278,  3132, 29915, 29879, 10823,   297,   278, 17759,   749,
          310,  2553, 29873,   470,  1592,   537,   409,  2764,  1907, 29889,
          530, 13258,   358,  9124,  1122,   884,  6985, 14582,  9701,   297,
         2778,  5743,   322,  1274,  7680,  2187,   313, 29924, 29987, 29909,
        29897,   322,  3867, 10359,   453,   653,  5786,  1316,   408,  9999,
         3907, 29892,  3534,   292,   310, 25748,   322,  1592,   537,   409,
         2764,  1907, 29892,   383,  2965, 29907,  5786,   313, 20227, 17869,
        23643, 29892, 16256, 15942, 29892,   322,   844,   397,  1907, 29897,
          470,  5925,   313, 25254, 29872,  4599,   293, 29892, 16200,   470,
         1592,   537,  5925,   467,  7849, 13258,   358, 24388,  7344,  6019,
         2545,  3946,   482,   322, 24342, 10643,  5840,  1860,   297,  9589,
          651,   411,  1009, 13258,   358,  5925,  5381,   267, 29889,  1094,
          385, 13661, 29892,   372,   338,  9391,   701,   964,   278,  8313,
          479,  5032,  3522,   313, 21064, 26485,   511, 14253, 28794,   313,
         6563, 29899,  5563,  5381,   267,   511,   322, 25927,  1387,  9999,
          313, 18732,  1891,  5381,   267,   467,    13,    13,  2525,  4561,
        12128, 24388,   322,  3240,   737, 24388, 29892, 13258,   358, 24388,
          437,   451,  2125, 19754,  1169, 29889,     2, 29871,    13]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1])}.
07/01/2024 01:27:00 - INFO - __main__ - trainable model_params: 134217728
07/01/2024 01:27:00 - INFO - __main__ - dist.is_initialized() and dist.get_rank() == 0
PeftModelForCausalLM(
  (base_model): LoraModel(
    (model): LlamaForCausalLM(
      (model): LlamaModel(
        (embed_tokens): Embedding(32001, 4096)
        (layers): ModuleList(
          (0-31): 32 x LlamaDecoderLayer(
            (self_attn): LlamaSdpaAttention(
              (q_proj): lora.Linear(
                (base_layer): Linear(in_features=4096, out_features=4096, bias=False)
                (lora_dropout): ModuleDict(
                  (default): Dropout(p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  (default): Linear(in_features=4096, out_features=128, bias=False)
                )
                (lora_B): ModuleDict(
                  (default): Linear(in_features=128, out_features=4096, bias=False)
                )
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
              )
              (k_proj): lora.Linear(
                (base_layer): Linear(in_features=4096, out_features=4096, bias=False)
                (lora_dropout): ModuleDict(
                  (default): Dropout(p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  (default): Linear(in_features=4096, out_features=128, bias=False)
                )
                (lora_B): ModuleDict(
                  (default): Linear(in_features=128, out_features=4096, bias=False)
                )
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
              )
              (v_proj): lora.Linear(
                (base_layer): Linear(in_features=4096, out_features=4096, bias=False)
                (lora_dropout): ModuleDict(
                  (default): Dropout(p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  (default): Linear(in_features=4096, out_features=128, bias=False)
                )
                (lora_B): ModuleDict(
                  (default): Linear(in_features=128, out_features=4096, bias=False)
                )
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
              )
              (o_proj): lora.Linear(
                (base_layer): Linear(in_features=4096, out_features=4096, bias=False)
                (lora_dropout): ModuleDict(
                  (default): Dropout(p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  (default): Linear(in_features=4096, out_features=128, bias=False)
                )
                (lora_B): ModuleDict(
                  (default): Linear(in_features=128, out_features=4096, bias=False)
                )
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
              )
              (rotary_emb): LlamaRotaryEmbedding()
            )
            (mlp): LlamaMLP(
              (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
              (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
              (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
              (act_fn): SiLU()
            )
            (input_layernorm): LlamaRMSNorm()
            (post_attention_layernorm): LlamaRMSNorm()
          )
        )
        (norm): LlamaRMSNorm()
      )
      (lm_head): Linear(in_features=4096, out_features=32001, bias=False)
    )
  )
)
/projects/illinois/eng/cs/haopeng/qirundai/anaconda3/envs/less/lib/python3.11/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False)
  warnings.warn(
07/01/2024 01:27:00 - WARNING - accelerate.utils.other - Detected kernel version 3.10.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[INFO|trainer.py:568] 2024-07-01 01:27:37,014 >> Using auto half precision backend
[INFO|trainer.py:1706] 2024-07-01 01:27:56,303 >> ***** Running training *****
[INFO|trainer.py:1707] 2024-07-01 01:27:56,304 >>   Num examples = 13,533
[INFO|trainer.py:1708] 2024-07-01 01:27:56,304 >>   Num Epochs = 4
[INFO|trainer.py:1709] 2024-07-01 01:27:56,304 >>   Instantaneous batch size per device = 1
[INFO|trainer.py:1712] 2024-07-01 01:27:56,304 >>   Total train batch size (w. parallel, distributed & accumulation) = 128
[INFO|trainer.py:1713] 2024-07-01 01:27:56,304 >>   Gradient Accumulation steps = 128
[INFO|trainer.py:1714] 2024-07-01 01:27:56,304 >>   Total optimization steps = 420
[INFO|trainer.py:1715] 2024-07-01 01:27:56,306 >>   Number of trainable parameters = 134,217,728
[INFO|integration_utils.py:722] 2024-07-01 01:27:56,308 >> Automatic Weights & Biases logging enabled, to disable set os.environ["WANDB_DISABLED"] = "true"
wandb: Currently logged in as: qirundai (raidriar_dai). Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.17.3
wandb: Run data is saved locally in /projects/illinois/eng/cs/haopeng/qirundai/LESS/wandb/run-20240701_012800-q8yi57rg
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run silvery-bush-1
wandb: ⭐️ View project at https://wandb.ai/raidriar_dai/LESS_Reproduce
wandb: 🚀 View run at https://wandb.ai/raidriar_dai/LESS_Reproduce/runs/q8yi57rg
  0%|          | 0/420 [00:00<?, ?it/s][WARNING|logging.py:314] 2024-07-01 01:28:15,495 >> You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
  0%|          | 1/420 [00:35<4:11:19, 35.99s/it]                                                 {'loss': 3.3157, 'learning_rate': 1.5384615384615387e-06, 'epoch': 0.01}
  0%|          | 1/420 [00:35<4:11:19, 35.99s/it]  0%|          | 2/420 [01:07<3:53:33, 33.52s/it]                                                 {'loss': 8.2263, 'learning_rate': 3.0769230769230774e-06, 'epoch': 0.02}
  0%|          | 2/420 [01:07<3:53:33, 33.52s/it]  1%|          | 3/420 [01:39<3:48:05, 32.82s/it]                                                 {'loss': 3.7632, 'learning_rate': 4.615384615384616e-06, 'epoch': 0.03}
  1%|          | 3/420 [01:39<3:48:05, 32.82s/it]  1%|          | 4/420 [02:11<3:43:24, 32.22s/it]                                                 {'loss': 3.7013, 'learning_rate': 6.153846153846155e-06, 'epoch': 0.04}
  1%|          | 4/420 [02:11<3:43:24, 32.22s/it]  1%|          | 5/420 [02:42<3:40:53, 31.93s/it]                                                 {'loss': 5.836, 'learning_rate': 7.692307692307694e-06, 'epoch': 0.05}
  1%|          | 5/420 [02:42<3:40:53, 31.93s/it]  1%|▏         | 6/420 [03:13<3:38:30, 31.67s/it]                                                 {'loss': 3.0883, 'learning_rate': 9.230769230769232e-06, 'epoch': 0.06}
  1%|▏         | 6/420 [03:13<3:38:30, 31.67s/it]  2%|▏         | 7/420 [03:46<3:39:43, 31.92s/it]                                                 {'loss': 8.8136, 'learning_rate': 1.076923076923077e-05, 'epoch': 0.07}
  2%|▏         | 7/420 [03:46<3:39:43, 31.92s/it]  2%|▏         | 8/420 [04:17<3:38:14, 31.78s/it]                                                 {'loss': 2.9809, 'learning_rate': 1.230769230769231e-05, 'epoch': 0.08}
  2%|▏         | 8/420 [04:17<3:38:14, 31.78s/it]  2%|▏         | 9/420 [04:49<3:38:50, 31.95s/it]                                                 {'loss': 3.6995, 'learning_rate': 1.3846153846153847e-05, 'epoch': 0.09}
  2%|▏         | 9/420 [04:49<3:38:50, 31.95s/it]  2%|▏         | 10/420 [05:21<3:37:16, 31.80s/it]                                                  {'loss': 2.6211, 'learning_rate': 1.5384615384615387e-05, 'epoch': 0.09}
  2%|▏         | 10/420 [05:21<3:37:16, 31.80s/it]  3%|▎         | 11/420 [05:53<3:36:43, 31.79s/it]                                                  {'loss': 2.195, 'learning_rate': 1.6923076923076924e-05, 'epoch': 0.1}
  3%|▎         | 11/420 [05:53<3:36:43, 31.79s/it]  3%|▎         | 12/420 [06:24<3:35:39, 31.71s/it]                                                  {'loss': 1.8855, 'learning_rate': 1.8461538461538465e-05, 'epoch': 0.11}
  3%|▎         | 12/420 [06:24<3:35:39, 31.71s/it]  3%|▎         | 13/420 [06:55<3:33:26, 31.47s/it]                                                  {'loss': 1.6496, 'learning_rate': 2e-05, 'epoch': 0.12}
  3%|▎         | 13/420 [06:55<3:33:26, 31.47s/it]  3%|▎         | 14/420 [07:27<3:34:41, 31.73s/it]                                                  {'loss': 1.5618, 'learning_rate': 1.9950859950859952e-05, 'epoch': 0.13}
  3%|▎         | 14/420 [07:27<3:34:41, 31.73s/it]  4%|▎         | 15/420 [07:59<3:32:54, 31.54s/it]                                                  {'loss': 1.4148, 'learning_rate': 1.9901719901719903e-05, 'epoch': 0.14}
  4%|▎         | 15/420 [07:59<3:32:54, 31.54s/it]  4%|▍         | 16/420 [08:30<3:31:37, 31.43s/it]                                                  {'loss': 1.7624, 'learning_rate': 1.9852579852579854e-05, 'epoch': 0.15}
  4%|▍         | 16/420 [08:30<3:31:37, 31.43s/it]  4%|▍         | 17/420 [09:02<3:32:41, 31.67s/it]                                                  {'loss': 1.3491, 'learning_rate': 1.9803439803439805e-05, 'epoch': 0.16}
  4%|▍         | 17/420 [09:02<3:32:41, 31.67s/it]  4%|▍         | 18/420 [09:33<3:31:53, 31.63s/it]                                                  {'loss': 1.9236, 'learning_rate': 1.9754299754299755e-05, 'epoch': 0.17}
  4%|▍         | 18/420 [09:33<3:31:53, 31.63s/it]  5%|▍         | 19/420 [10:06<3:32:24, 31.78s/it]                                                  {'loss': 1.2841, 'learning_rate': 1.9705159705159706e-05, 'epoch': 0.18}
  5%|▍         | 19/420 [10:06<3:32:24, 31.78s/it]  5%|▍         | 20/420 [10:37<3:31:20, 31.70s/it]                                                  {'loss': 1.1978, 'learning_rate': 1.9656019656019657e-05, 'epoch': 0.19}
  5%|▍         | 20/420 [10:37<3:31:20, 31.70s/it]  5%|▌         | 21/420 [11:09<3:31:31, 31.81s/it]                                                  {'loss': 1.1579, 'learning_rate': 1.9606879606879607e-05, 'epoch': 0.2}
  5%|▌         | 21/420 [11:09<3:31:31, 31.81s/it]  5%|▌         | 22/420 [11:40<3:30:05, 31.67s/it]                                                  {'loss': 2.2229, 'learning_rate': 1.9557739557739558e-05, 'epoch': 0.21}
  5%|▌         | 22/420 [11:40<3:30:05, 31.67s/it]  5%|▌         | 23/420 [12:11<3:28:07, 31.45s/it]                                                  {'loss': 1.2243, 'learning_rate': 1.950859950859951e-05, 'epoch': 0.22}
  5%|▌         | 23/420 [12:11<3:28:07, 31.45s/it]  6%|▌         | 24/420 [12:42<3:26:45, 31.33s/it]                                                  {'loss': 1.228, 'learning_rate': 1.9459459459459463e-05, 'epoch': 0.23}
  6%|▌         | 24/420 [12:42<3:26:45, 31.33s/it]  6%|▌         | 25/420 [13:14<3:25:41, 31.24s/it]                                                  {'loss': 1.202, 'learning_rate': 1.941031941031941e-05, 'epoch': 0.24}
  6%|▌         | 25/420 [13:14<3:25:41, 31.24s/it]  6%|▌         | 26/420 [13:45<3:25:57, 31.36s/it]                                                  {'loss': 1.3606, 'learning_rate': 1.9361179361179365e-05, 'epoch': 0.25}
  6%|▌         | 26/420 [13:45<3:25:57, 31.36s/it]  6%|▋         | 27/420 [14:16<3:24:44, 31.26s/it]                                                  {'loss': 1.1862, 'learning_rate': 1.9312039312039315e-05, 'epoch': 0.26}
  6%|▋         | 27/420 [14:16<3:24:44, 31.26s/it]  7%|▋         | 28/420 [14:47<3:22:53, 31.05s/it]                                                  {'loss': 1.1073, 'learning_rate': 1.9262899262899263e-05, 'epoch': 0.26}
  7%|▋         | 28/420 [14:47<3:22:53, 31.05s/it]  7%|▋         | 29/420 [15:19<3:24:31, 31.38s/it]                                                  {'loss': 1.1738, 'learning_rate': 1.9213759213759217e-05, 'epoch': 0.27}
  7%|▋         | 29/420 [15:19<3:24:31, 31.38s/it]  7%|▋         | 30/420 [15:50<3:23:36, 31.32s/it]                                                  {'loss': 1.2665, 'learning_rate': 1.9164619164619167e-05, 'epoch': 0.28}
  7%|▋         | 30/420 [15:50<3:23:36, 31.32s/it]  7%|▋         | 31/420 [16:21<3:22:40, 31.26s/it]                                                  {'loss': 1.1534, 'learning_rate': 1.9115479115479115e-05, 'epoch': 0.29}
  7%|▋         | 31/420 [16:21<3:22:40, 31.26s/it]  8%|▊         | 32/420 [16:52<3:22:00, 31.24s/it]                                                  {'loss': 1.7264, 'learning_rate': 1.906633906633907e-05, 'epoch': 0.3}
  8%|▊         | 32/420 [16:52<3:22:00, 31.24s/it]  8%|▊         | 33/420 [17:23<3:20:45, 31.13s/it]                                                  {'loss': 1.0753, 'learning_rate': 1.901719901719902e-05, 'epoch': 0.31}
  8%|▊         | 33/420 [17:23<3:20:45, 31.13s/it]  8%|▊         | 34/420 [17:54<3:18:51, 30.91s/it]                                                  {'loss': 1.0335, 'learning_rate': 1.896805896805897e-05, 'epoch': 0.32}
  8%|▊         | 34/420 [17:54<3:18:51, 30.91s/it]  8%|▊         | 35/420 [18:24<3:16:21, 30.60s/it]                                                  {'loss': 1.0216, 'learning_rate': 1.891891891891892e-05, 'epoch': 0.33}
  8%|▊         | 35/420 [18:24<3:16:21, 30.60s/it]  9%|▊         | 36/420 [18:56<3:18:42, 31.05s/it]                                                  {'loss': 2.0673, 'learning_rate': 1.8869778869778872e-05, 'epoch': 0.34}
  9%|▊         | 36/420 [18:56<3:18:42, 31.05s/it]  9%|▉         | 37/420 [19:26<3:17:03, 30.87s/it]                                                  {'loss': 1.0147, 'learning_rate': 1.8820638820638823e-05, 'epoch': 0.35}
  9%|▉         | 37/420 [19:26<3:17:03, 30.87s/it]  9%|▉         | 38/420 [19:57<3:16:04, 30.80s/it]                                                  {'loss': 1.0614, 'learning_rate': 1.8771498771498773e-05, 'epoch': 0.36}
  9%|▉         | 38/420 [19:57<3:16:04, 30.80s/it]  9%|▉         | 39/420 [20:28<3:15:35, 30.80s/it]                                                  {'loss': 1.0966, 'learning_rate': 1.8722358722358724e-05, 'epoch': 0.37}
  9%|▉         | 39/420 [20:28<3:15:35, 30.80s/it] 10%|▉         | 40/420 [20:58<3:15:21, 30.85s/it]                                                  {'loss': 1.2683, 'learning_rate': 1.8673218673218675e-05, 'epoch': 0.38}
 10%|▉         | 40/420 [20:58<3:15:21, 30.85s/it] 10%|▉         | 41/420 [21:30<3:15:47, 31.00s/it]                                                  {'loss': 1.159, 'learning_rate': 1.8624078624078625e-05, 'epoch': 0.39}
 10%|▉         | 41/420 [21:30<3:15:47, 31.00s/it] 10%|█         | 42/420 [22:01<3:15:55, 31.10s/it]                                                  {'loss': 1.126, 'learning_rate': 1.8574938574938576e-05, 'epoch': 0.4}
 10%|█         | 42/420 [22:01<3:15:55, 31.10s/it] 10%|█         | 43/420 [22:33<3:16:35, 31.29s/it]                                                  {'loss': 1.1345, 'learning_rate': 1.8525798525798527e-05, 'epoch': 0.41}
 10%|█         | 43/420 [22:33<3:16:35, 31.29s/it] 10%|█         | 44/420 [23:04<3:16:34, 31.37s/it]                                                  {'loss': 1.1587, 'learning_rate': 1.8476658476658478e-05, 'epoch': 0.42}
 10%|█         | 44/420 [23:04<3:16:34, 31.37s/it] 11%|█         | 45/420 [23:35<3:15:04, 31.21s/it]                                                  {'loss': 1.053, 'learning_rate': 1.842751842751843e-05, 'epoch': 0.43}
 11%|█         | 45/420 [23:35<3:15:04, 31.21s/it] 11%|█         | 46/420 [24:06<3:14:16, 31.17s/it]                                                  {'loss': 0.9917, 'learning_rate': 1.8378378378378383e-05, 'epoch': 0.44}
 11%|█         | 46/420 [24:06<3:14:16, 31.17s/it] 11%|█         | 47/420 [24:38<3:14:41, 31.32s/it]                                                  {'loss': 1.1027, 'learning_rate': 1.832923832923833e-05, 'epoch': 0.44}
 11%|█         | 47/420 [24:38<3:14:41, 31.32s/it] 11%|█▏        | 48/420 [25:09<3:13:25, 31.20s/it]                                                  {'loss': 1.5536, 'learning_rate': 1.828009828009828e-05, 'epoch': 0.45}
 11%|█▏        | 48/420 [25:09<3:13:25, 31.20s/it] 12%|█▏        | 49/420 [25:41<3:13:43, 31.33s/it]                                                  {'loss': 1.0046, 'learning_rate': 1.8230958230958235e-05, 'epoch': 0.46}
 12%|█▏        | 49/420 [25:41<3:13:43, 31.33s/it] 12%|█▏        | 50/420 [26:13<3:14:54, 31.61s/it]                                                  {'loss': 3.1895, 'learning_rate': 1.8181818181818182e-05, 'epoch': 0.47}
 12%|█▏        | 50/420 [26:13<3:14:54, 31.61s/it] 12%|█▏        | 51/420 [26:44<3:13:37, 31.49s/it]                                                  {'loss': 1.0501, 'learning_rate': 1.8132678132678133e-05, 'epoch': 0.48}
 12%|█▏        | 51/420 [26:44<3:13:37, 31.49s/it] 12%|█▏        | 52/420 [27:15<3:11:55, 31.29s/it]                                                  {'loss': 1.043, 'learning_rate': 1.8083538083538087e-05, 'epoch': 0.49}
 12%|█▏        | 52/420 [27:15<3:11:55, 31.29s/it] 13%|█▎        | 53/420 [27:46<3:10:41, 31.17s/it]                                                  {'loss': 1.0443, 'learning_rate': 1.8034398034398034e-05, 'epoch': 0.5}
 13%|█▎        | 53/420 [27:46<3:10:41, 31.17s/it] 13%|█▎        | 54/420 [28:16<3:09:15, 31.03s/it]                                                  {'loss': 1.0704, 'learning_rate': 1.798525798525799e-05, 'epoch': 0.51}
 13%|█▎        | 54/420 [28:16<3:09:15, 31.03s/it] 13%|█▎        | 55/420 [28:48<3:09:29, 31.15s/it]                                                  {'loss': 1.1542, 'learning_rate': 1.793611793611794e-05, 'epoch': 0.52}
 13%|█▎        | 55/420 [28:48<3:09:29, 31.15s/it] 13%|█▎        | 56/420 [29:18<3:07:49, 30.96s/it]                                                  {'loss': 1.1424, 'learning_rate': 1.7886977886977886e-05, 'epoch': 0.53}
 13%|█▎        | 56/420 [29:18<3:07:49, 30.96s/it] 14%|█▎        | 57/420 [29:49<3:06:48, 30.88s/it]                                                  {'loss': 0.9692, 'learning_rate': 1.783783783783784e-05, 'epoch': 0.54}
 14%|█▎        | 57/420 [29:49<3:06:48, 30.88s/it] 14%|█▍        | 58/420 [30:20<3:05:46, 30.79s/it]                                                  {'loss': 1.469, 'learning_rate': 1.778869778869779e-05, 'epoch': 0.55}
 14%|█▍        | 58/420 [30:20<3:05:46, 30.79s/it] 14%|█▍        | 59/420 [30:51<3:06:03, 30.92s/it]                                                  {'loss': 1.0467, 'learning_rate': 1.773955773955774e-05, 'epoch': 0.56}
 14%|█▍        | 59/420 [30:51<3:06:03, 30.92s/it] 14%|█▍        | 60/420 [31:22<3:05:05, 30.85s/it]                                                  {'loss': 1.0936, 'learning_rate': 1.7690417690417693e-05, 'epoch': 0.57}
 14%|█▍        | 60/420 [31:22<3:05:05, 30.85s/it] 15%|█▍        | 61/420 [31:52<3:03:50, 30.73s/it]                                                  {'loss': 1.0399, 'learning_rate': 1.7641277641277644e-05, 'epoch': 0.58}
 15%|█▍        | 61/420 [31:52<3:03:50, 30.73s/it] 15%|█▍        | 62/420 [32:23<3:03:34, 30.77s/it]                                                  {'loss': 1.964, 'learning_rate': 1.759213759213759e-05, 'epoch': 0.59}
 15%|█▍        | 62/420 [32:23<3:03:34, 30.77s/it] 15%|█▌        | 63/420 [32:54<3:03:17, 30.80s/it]                                                  {'loss': 1.1019, 'learning_rate': 1.7542997542997545e-05, 'epoch': 0.6}
 15%|█▌        | 63/420 [32:54<3:03:17, 30.80s/it] 15%|█▌        | 64/420 [33:25<3:04:14, 31.05s/it]                                                  {'loss': 1.0888, 'learning_rate': 1.7493857493857496e-05, 'epoch': 0.61}
 15%|█▌        | 64/420 [33:25<3:04:14, 31.05s/it] 15%|█▌        | 65/420 [33:56<3:03:46, 31.06s/it]                                                  {'loss': 1.568, 'learning_rate': 1.7444717444717446e-05, 'epoch': 0.61}
 15%|█▌        | 65/420 [33:56<3:03:46, 31.06s/it] 16%|█▌        | 66/420 [34:28<3:03:32, 31.11s/it]                                                  {'loss': 1.0156, 'learning_rate': 1.7395577395577397e-05, 'epoch': 0.62}
 16%|█▌        | 66/420 [34:28<3:03:32, 31.11s/it] 16%|█▌        | 67/420 [34:59<3:04:07, 31.30s/it]                                                  {'loss': 0.9629, 'learning_rate': 1.7346437346437348e-05, 'epoch': 0.63}
 16%|█▌        | 67/420 [34:59<3:04:07, 31.30s/it] 16%|█▌        | 68/420 [35:30<3:03:06, 31.21s/it]                                                  {'loss': 1.1611, 'learning_rate': 1.72972972972973e-05, 'epoch': 0.64}
 16%|█▌        | 68/420 [35:30<3:03:06, 31.21s/it] 16%|█▋        | 69/420 [36:02<3:02:47, 31.25s/it]                                                  {'loss': 1.2384, 'learning_rate': 1.724815724815725e-05, 'epoch': 0.65}
 16%|█▋        | 69/420 [36:02<3:02:47, 31.25s/it] 17%|█▋        | 70/420 [36:33<3:03:05, 31.39s/it]                                                  {'loss': 1.5933, 'learning_rate': 1.71990171990172e-05, 'epoch': 0.66}
 17%|█▋        | 70/420 [36:34<3:03:05, 31.39s/it] 17%|█▋        | 71/420 [37:04<3:01:51, 31.26s/it]                                                  {'loss': 0.9833, 'learning_rate': 1.714987714987715e-05, 'epoch': 0.67}
 17%|█▋        | 71/420 [37:04<3:01:51, 31.26s/it] 17%|█▋        | 72/420 [37:36<3:02:19, 31.43s/it]                                                  {'loss': 1.026, 'learning_rate': 1.71007371007371e-05, 'epoch': 0.68}
 17%|█▋        | 72/420 [37:36<3:02:19, 31.43s/it] 17%|█▋        | 73/420 [38:07<3:00:36, 31.23s/it]                                                  {'loss': 0.9911, 'learning_rate': 1.7051597051597052e-05, 'epoch': 0.69}
 17%|█▋        | 73/420 [38:07<3:00:36, 31.23s/it] 18%|█▊        | 74/420 [38:38<2:59:44, 31.17s/it]                                                  {'loss': 1.082, 'learning_rate': 1.7002457002457003e-05, 'epoch': 0.7}
 18%|█▊        | 74/420 [38:38<2:59:44, 31.17s/it] 18%|█▊        | 75/420 [39:10<3:00:11, 31.34s/it]                                                  {'loss': 0.9652, 'learning_rate': 1.6953316953316954e-05, 'epoch': 0.71}
 18%|█▊        | 75/420 [39:10<3:00:11, 31.34s/it] 18%|█▊        | 76/420 [39:40<2:57:58, 31.04s/it]                                                  {'loss': 1.1203, 'learning_rate': 1.6904176904176904e-05, 'epoch': 0.72}
 18%|█▊        | 76/420 [39:40<2:57:58, 31.04s/it] 18%|█▊        | 77/420 [40:11<2:57:52, 31.11s/it]                                                  {'loss': 0.9281, 'learning_rate': 1.685503685503686e-05, 'epoch': 0.73}
 18%|█▊        | 77/420 [40:11<2:57:52, 31.11s/it] 19%|█▊        | 78/420 [40:44<2:59:22, 31.47s/it]                                                  {'loss': 0.9362, 'learning_rate': 1.6805896805896806e-05, 'epoch': 0.74}
 19%|█▊        | 78/420 [40:44<2:59:22, 31.47s/it] 19%|█▉        | 79/420 [41:14<2:57:29, 31.23s/it]                                                  {'loss': 1.0032, 'learning_rate': 1.6756756756756757e-05, 'epoch': 0.75}
 19%|█▉        | 79/420 [41:14<2:57:29, 31.23s/it] 19%|█▉        | 80/420 [41:45<2:56:18, 31.11s/it]                                                  {'loss': 1.0117, 'learning_rate': 1.670761670761671e-05, 'epoch': 0.76}
 19%|█▉        | 80/420 [41:45<2:56:18, 31.11s/it] 19%|█▉        | 81/420 [42:16<2:55:20, 31.03s/it]                                                  {'loss': 1.0814, 'learning_rate': 1.6658476658476658e-05, 'epoch': 0.77}
 19%|█▉        | 81/420 [42:16<2:55:20, 31.03s/it] 20%|█▉        | 82/420 [42:48<2:55:42, 31.19s/it]                                                  {'loss': 0.9576, 'learning_rate': 1.660933660933661e-05, 'epoch': 0.78}
 20%|█▉        | 82/420 [42:48<2:55:42, 31.19s/it] 20%|█▉        | 83/420 [43:20<2:56:38, 31.45s/it]                                                  {'loss': 1.3442, 'learning_rate': 1.6560196560196563e-05, 'epoch': 0.79}
 20%|█▉        | 83/420 [43:20<2:56:38, 31.45s/it] 20%|██        | 84/420 [43:51<2:55:17, 31.30s/it]                                                  {'loss': 1.0368, 'learning_rate': 1.651105651105651e-05, 'epoch': 0.79}
 20%|██        | 84/420 [43:51<2:55:17, 31.30s/it] 20%|██        | 85/420 [44:22<2:53:58, 31.16s/it]                                                  {'loss': 1.2403, 'learning_rate': 1.6461916461916464e-05, 'epoch': 0.8}
 20%|██        | 85/420 [44:22<2:53:58, 31.16s/it] 20%|██        | 86/420 [44:53<2:53:22, 31.14s/it]                                                  {'loss': 1.0291, 'learning_rate': 1.6412776412776415e-05, 'epoch': 0.81}
 20%|██        | 86/420 [44:53<2:53:22, 31.14s/it] 21%|██        | 87/420 [45:23<2:52:18, 31.05s/it]                                                  {'loss': 1.0844, 'learning_rate': 1.6363636363636366e-05, 'epoch': 0.82}
 21%|██        | 87/420 [45:23<2:52:18, 31.05s/it] 21%|██        | 88/420 [45:55<2:52:29, 31.17s/it]                                                  {'loss': 1.4006, 'learning_rate': 1.6314496314496317e-05, 'epoch': 0.83}
 21%|██        | 88/420 [45:55<2:52:29, 31.17s/it] 21%|██        | 89/420 [46:25<2:50:41, 30.94s/it]                                                  {'loss': 1.0089, 'learning_rate': 1.6265356265356267e-05, 'epoch': 0.84}
 21%|██        | 89/420 [46:25<2:50:41, 30.94s/it] 21%|██▏       | 90/420 [46:56<2:49:04, 30.74s/it]                                                  {'loss': 0.9042, 'learning_rate': 1.6216216216216218e-05, 'epoch': 0.85}
 21%|██▏       | 90/420 [46:56<2:49:04, 30.74s/it] 22%|██▏       | 91/420 [47:26<2:47:43, 30.59s/it]                                                  {'loss': 0.9592, 'learning_rate': 1.616707616707617e-05, 'epoch': 0.86}
 22%|██▏       | 91/420 [47:26<2:47:43, 30.59s/it] 22%|██▏       | 92/420 [47:58<2:50:21, 31.16s/it]                                                  {'loss': 1.2025, 'learning_rate': 1.611793611793612e-05, 'epoch': 0.87}
 22%|██▏       | 92/420 [47:58<2:50:21, 31.16s/it] 22%|██▏       | 93/420 [48:29<2:49:47, 31.15s/it]                                                  {'loss': 0.9585, 'learning_rate': 1.606879606879607e-05, 'epoch': 0.88}
 22%|██▏       | 93/420 [48:29<2:49:47, 31.15s/it] 22%|██▏       | 94/420 [49:01<2:49:43, 31.24s/it]                                                  {'loss': 0.9418, 'learning_rate': 1.601965601965602e-05, 'epoch': 0.89}
 22%|██▏       | 94/420 [49:01<2:49:43, 31.24s/it] 23%|██▎       | 95/420 [49:33<2:50:07, 31.41s/it]                                                  {'loss': 1.0523, 'learning_rate': 1.5970515970515972e-05, 'epoch': 0.9}
 23%|██▎       | 95/420 [49:33<2:50:07, 31.41s/it] 23%|██▎       | 96/420 [50:04<2:49:04, 31.31s/it]                                                  {'loss': 1.0876, 'learning_rate': 1.5921375921375922e-05, 'epoch': 0.91}
 23%|██▎       | 96/420 [50:04<2:49:04, 31.31s/it] 23%|██▎       | 97/420 [50:35<2:49:06, 31.41s/it]                                                  {'loss': 1.9589, 'learning_rate': 1.5872235872235873e-05, 'epoch': 0.92}
 23%|██▎       | 97/420 [50:35<2:49:06, 31.41s/it] 23%|██▎       | 98/420 [51:06<2:47:48, 31.27s/it]                                                  {'loss': 0.9677, 'learning_rate': 1.5823095823095824e-05, 'epoch': 0.93}
 23%|██▎       | 98/420 [51:06<2:47:48, 31.27s/it] 24%|██▎       | 99/420 [51:39<2:48:49, 31.56s/it]                                                  {'loss': 1.1084, 'learning_rate': 1.5773955773955775e-05, 'epoch': 0.94}
 24%|██▎       | 99/420 [51:39<2:48:49, 31.56s/it] 24%|██▍       | 100/420 [52:11<2:49:00, 31.69s/it]                                                   {'loss': 0.9459, 'learning_rate': 1.5724815724815725e-05, 'epoch': 0.95}
 24%|██▍       | 100/420 [52:11<2:49:00, 31.69s/it] 24%|██▍       | 101/420 [52:43<2:48:59, 31.79s/it]                                                   {'loss': 0.9977, 'learning_rate': 1.5675675675675676e-05, 'epoch': 0.96}
 24%|██▍       | 101/420 [52:43<2:48:59, 31.79s/it] 24%|██▍       | 102/420 [53:14<2:47:48, 31.66s/it]                                                   {'loss': 0.9265, 'learning_rate': 1.5626535626535627e-05, 'epoch': 0.96}
 24%|██▍       | 102/420 [53:14<2:47:48, 31.66s/it] 25%|██▍       | 103/420 [53:45<2:46:37, 31.54s/it]                                                   {'loss': 0.9746, 'learning_rate': 1.5577395577395578e-05, 'epoch': 0.97}
 25%|██▍       | 103/420 [53:45<2:46:37, 31.54s/it] 25%|██▍       | 104/420 [54:17<2:46:06, 31.54s/it]                                                   {'loss': 0.9057, 'learning_rate': 1.552825552825553e-05, 'epoch': 0.98}
 25%|██▍       | 104/420 [54:17<2:46:06, 31.54s/it] 25%|██▌       | 105/420 [54:49<2:46:12, 31.66s/it]                                                   {'loss': 0.9162, 'learning_rate': 1.547911547911548e-05, 'epoch': 0.99}
 25%|██▌       | 105/420 [54:49<2:46:12, 31.66s/it][INFO|trainer.py:2889] 2024-07-01 02:23:27,996 >> Saving model checkpoint to ../out/Llama-2-7b-p0.05-lora-seed3-bsz128/tmp-checkpoint-105
[INFO|tokenization_utils_base.py:2432] 2024-07-01 02:23:31,829 >> tokenizer config file saved in ../out/Llama-2-7b-p0.05-lora-seed3-bsz128/tmp-checkpoint-105/tokenizer_config.json
[INFO|tokenization_utils_base.py:2441] 2024-07-01 02:23:31,830 >> Special tokens file saved in ../out/Llama-2-7b-p0.05-lora-seed3-bsz128/tmp-checkpoint-105/special_tokens_map.json
 25%|██▌       | 106/420 [56:31<4:36:30, 52.84s/it]                                                   {'loss': 1.6583, 'learning_rate': 1.5429975429975433e-05, 'epoch': 1.0}
 25%|██▌       | 106/420 [56:31<4:36:30, 52.84s/it] 25%|██▌       | 107/420 [57:02<4:01:50, 46.36s/it]                                                   {'loss': 0.887, 'learning_rate': 1.538083538083538e-05, 'epoch': 1.01}
 25%|██▌       | 107/420 [57:02<4:01:50, 46.36s/it] 26%|██▌       | 108/420 [57:33<3:36:54, 41.71s/it]                                                   {'loss': 0.9234, 'learning_rate': 1.5331695331695335e-05, 'epoch': 1.02}
 26%|██▌       | 108/420 [57:33<3:36:54, 41.71s/it] 26%|██▌       | 109/420 [58:05<3:20:51, 38.75s/it]                                                   {'loss': 1.3732, 'learning_rate': 1.5282555282555285e-05, 'epoch': 1.03}
 26%|██▌       | 109/420 [58:05<3:20:51, 38.75s/it] 26%|██▌       | 110/420 [58:37<3:09:40, 36.71s/it]                                                   {'loss': 1.0034, 'learning_rate': 1.5233415233415234e-05, 'epoch': 1.04}
 26%|██▌       | 110/420 [58:37<3:09:40, 36.71s/it] 26%|██▋       | 111/420 [59:09<3:01:19, 35.21s/it]                                                   {'loss': 1.028, 'learning_rate': 1.5184275184275185e-05, 'epoch': 1.05}
 26%|██▋       | 111/420 [59:09<3:01:19, 35.21s/it] 27%|██▋       | 112/420 [59:39<2:54:02, 33.90s/it]                                                   {'loss': 1.0154, 'learning_rate': 1.5135135135135138e-05, 'epoch': 1.06}
 27%|██▋       | 112/420 [59:39<2:54:02, 33.90s/it] 27%|██▋       | 113/420 [1:00:12<2:51:14, 33.47s/it]                                                     {'loss': 2.3224, 'learning_rate': 1.5085995085995087e-05, 'epoch': 1.07}
 27%|██▋       | 113/420 [1:00:12<2:51:14, 33.47s/it] 27%|██▋       | 114/420 [1:00:42<2:45:58, 32.54s/it]                                                     {'loss': 1.0236, 'learning_rate': 1.5036855036855037e-05, 'epoch': 1.08}
 27%|██▋       | 114/420 [1:00:42<2:45:58, 32.54s/it] 27%|██▋       | 115/420 [1:01:13<2:43:21, 32.14s/it]                                                     {'loss': 0.8821, 'learning_rate': 1.498771498771499e-05, 'epoch': 1.09}
 27%|██▋       | 115/420 [1:01:13<2:43:21, 32.14s/it] 28%|██▊       | 116/420 [1:01:45<2:42:19, 32.04s/it]                                                     {'loss': 0.8862, 'learning_rate': 1.4938574938574939e-05, 'epoch': 1.1}
 28%|██▊       | 116/420 [1:01:45<2:42:19, 32.04s/it] 28%|██▊       | 117/420 [1:02:17<2:41:32, 31.99s/it]                                                     {'loss': 0.9328, 'learning_rate': 1.4889434889434891e-05, 'epoch': 1.11}
 28%|██▊       | 117/420 [1:02:17<2:41:32, 31.99s/it] 28%|██▊       | 118/420 [1:02:48<2:38:39, 31.52s/it]                                                     {'loss': 1.0289, 'learning_rate': 1.4840294840294842e-05, 'epoch': 1.12}
 28%|██▊       | 118/420 [1:02:48<2:38:39, 31.52s/it] 28%|██▊       | 119/420 [1:03:19<2:38:20, 31.56s/it]                                                     {'loss': 0.8383, 'learning_rate': 1.4791154791154791e-05, 'epoch': 1.13}
 28%|██▊       | 119/420 [1:03:19<2:38:20, 31.56s/it] 29%|██▊       | 120/420 [1:03:50<2:36:22, 31.28s/it]                                                     {'loss': 0.9918, 'learning_rate': 1.4742014742014743e-05, 'epoch': 1.14}
 29%|██▊       | 120/420 [1:03:50<2:36:22, 31.28s/it] 29%|██▉       | 121/420 [1:04:22<2:36:50, 31.47s/it]                                                     {'loss': 0.9348, 'learning_rate': 1.4692874692874694e-05, 'epoch': 1.14}
 29%|██▉       | 121/420 [1:04:22<2:36:50, 31.47s/it] 29%|██▉       | 122/420 [1:04:53<2:36:24, 31.49s/it]                                                     {'loss': 0.9307, 'learning_rate': 1.4643734643734643e-05, 'epoch': 1.15}
 29%|██▉       | 122/420 [1:04:53<2:36:24, 31.49s/it] 29%|██▉       | 123/420 [1:05:25<2:35:53, 31.49s/it]                                                     {'loss': 0.9911, 'learning_rate': 1.4594594594594596e-05, 'epoch': 1.16}
 29%|██▉       | 123/420 [1:05:25<2:35:53, 31.49s/it] 30%|██▉       | 124/420 [1:05:56<2:35:05, 31.44s/it]                                                     {'loss': 0.9149, 'learning_rate': 1.4545454545454546e-05, 'epoch': 1.17}
 30%|██▉       | 124/420 [1:05:56<2:35:05, 31.44s/it] 30%|██▉       | 125/420 [1:06:28<2:35:20, 31.60s/it]                                                     {'loss': 1.3487, 'learning_rate': 1.4496314496314499e-05, 'epoch': 1.18}
 30%|██▉       | 125/420 [1:06:28<2:35:20, 31.60s/it] 30%|███       | 126/420 [1:07:00<2:34:46, 31.59s/it]                                                     {'loss': 1.0188, 'learning_rate': 1.4447174447174448e-05, 'epoch': 1.19}
 30%|███       | 126/420 [1:07:00<2:34:46, 31.59s/it] 30%|███       | 127/420 [1:07:31<2:34:04, 31.55s/it]                                                     {'loss': 0.858, 'learning_rate': 1.43980343980344e-05, 'epoch': 1.2}
 30%|███       | 127/420 [1:07:31<2:34:04, 31.55s/it] 30%|███       | 128/420 [1:08:03<2:34:26, 31.74s/it]                                                     {'loss': 0.9636, 'learning_rate': 1.4348894348894351e-05, 'epoch': 1.21}
 30%|███       | 128/420 [1:08:03<2:34:26, 31.74s/it] 31%|███       | 129/420 [1:08:34<2:32:53, 31.52s/it]                                                     {'loss': 1.4777, 'learning_rate': 1.42997542997543e-05, 'epoch': 1.22}
 31%|███       | 129/420 [1:08:34<2:32:53, 31.52s/it] 31%|███       | 130/420 [1:09:06<2:32:33, 31.56s/it]                                                     {'loss': 1.0407, 'learning_rate': 1.4250614250614252e-05, 'epoch': 1.23}
 31%|███       | 130/420 [1:09:06<2:32:33, 31.56s/it] 31%|███       | 131/420 [1:09:37<2:31:32, 31.46s/it]                                                     {'loss': 0.9365, 'learning_rate': 1.4201474201474203e-05, 'epoch': 1.24}
 31%|███       | 131/420 [1:09:37<2:31:32, 31.46s/it] 31%|███▏      | 132/420 [1:10:08<2:30:41, 31.39s/it]                                                     {'loss': 1.55, 'learning_rate': 1.4152334152334152e-05, 'epoch': 1.25}
 31%|███▏      | 132/420 [1:10:08<2:30:41, 31.39s/it] 32%|███▏      | 133/420 [1:10:40<2:29:58, 31.35s/it]                                                     {'loss': 0.982, 'learning_rate': 1.4103194103194105e-05, 'epoch': 1.26}
 32%|███▏      | 133/420 [1:10:40<2:29:58, 31.35s/it] 32%|███▏      | 134/420 [1:11:11<2:30:01, 31.48s/it]                                                     {'loss': 0.8626, 'learning_rate': 1.4054054054054055e-05, 'epoch': 1.27}
 32%|███▏      | 134/420 [1:11:11<2:30:01, 31.48s/it] 32%|███▏      | 135/420 [1:11:43<2:29:52, 31.55s/it]                                                     {'loss': 1.1731, 'learning_rate': 1.4004914004914006e-05, 'epoch': 1.28}
 32%|███▏      | 135/420 [1:11:43<2:29:52, 31.55s/it] 32%|███▏      | 136/420 [1:12:15<2:29:21, 31.55s/it]                                                     {'loss': 1.7627, 'learning_rate': 1.3955773955773957e-05, 'epoch': 1.29}
 32%|███▏      | 136/420 [1:12:15<2:29:21, 31.55s/it] 33%|███▎      | 137/420 [1:12:47<2:29:43, 31.74s/it]                                                     {'loss': 0.9127, 'learning_rate': 1.390663390663391e-05, 'epoch': 1.3}
 33%|███▎      | 137/420 [1:12:47<2:29:43, 31.74s/it] 33%|███▎      | 138/420 [1:13:19<2:29:13, 31.75s/it]                                                     {'loss': 0.9336, 'learning_rate': 1.3857493857493858e-05, 'epoch': 1.31}
 33%|███▎      | 138/420 [1:13:19<2:29:13, 31.75s/it] 33%|███▎      | 139/420 [1:13:51<2:29:12, 31.86s/it]                                                     {'loss': 0.9515, 'learning_rate': 1.3808353808353809e-05, 'epoch': 1.31}
 33%|███▎      | 139/420 [1:13:51<2:29:12, 31.86s/it] 33%|███▎      | 140/420 [1:14:22<2:28:24, 31.80s/it]                                                     {'loss': 0.9428, 'learning_rate': 1.3759213759213761e-05, 'epoch': 1.32}
 33%|███▎      | 140/420 [1:14:22<2:28:24, 31.80s/it] 34%|███▎      | 141/420 [1:14:55<2:28:22, 31.91s/it]                                                     {'loss': 0.9011, 'learning_rate': 1.371007371007371e-05, 'epoch': 1.33}
 34%|███▎      | 141/420 [1:14:55<2:28:22, 31.91s/it] 34%|███▍      | 142/420 [1:15:27<2:27:59, 31.94s/it]                                                     {'loss': 0.9516, 'learning_rate': 1.3660933660933661e-05, 'epoch': 1.34}
 34%|███▍      | 142/420 [1:15:27<2:27:59, 31.94s/it] 34%|███▍      | 143/420 [1:15:58<2:26:20, 31.70s/it]                                                     {'loss': 0.9762, 'learning_rate': 1.3611793611793614e-05, 'epoch': 1.35}
 34%|███▍      | 143/420 [1:15:58<2:26:20, 31.70s/it] 34%|███▍      | 144/420 [1:16:29<2:25:01, 31.53s/it]                                                     {'loss': 0.9056, 'learning_rate': 1.3562653562653563e-05, 'epoch': 1.36}
 34%|███▍      | 144/420 [1:16:29<2:25:01, 31.53s/it] 35%|███▍      | 145/420 [1:17:01<2:24:55, 31.62s/it]                                                     {'loss': 0.9284, 'learning_rate': 1.3513513513513515e-05, 'epoch': 1.37}
 35%|███▍      | 145/420 [1:17:01<2:24:55, 31.62s/it] 35%|███▍      | 146/420 [1:17:33<2:24:56, 31.74s/it]                                                     {'loss': 0.9179, 'learning_rate': 1.3464373464373466e-05, 'epoch': 1.38}
 35%|███▍      | 146/420 [1:17:33<2:24:56, 31.74s/it] 35%|███▌      | 147/420 [1:18:04<2:24:17, 31.71s/it]                                                     {'loss': 0.9558, 'learning_rate': 1.3415233415233417e-05, 'epoch': 1.39}
 35%|███▌      | 147/420 [1:18:04<2:24:17, 31.71s/it] 35%|███▌      | 148/420 [1:18:36<2:23:33, 31.67s/it]                                                     {'loss': 0.9185, 'learning_rate': 1.3366093366093367e-05, 'epoch': 1.4}
 35%|███▌      | 148/420 [1:18:36<2:23:33, 31.67s/it] 35%|███▌      | 149/420 [1:19:08<2:23:45, 31.83s/it]                                                     {'loss': 0.9975, 'learning_rate': 1.3316953316953318e-05, 'epoch': 1.41}
 35%|███▌      | 149/420 [1:19:08<2:23:45, 31.83s/it] 36%|███▌      | 150/420 [1:19:41<2:24:18, 32.07s/it]                                                     {'loss': 1.2536, 'learning_rate': 1.326781326781327e-05, 'epoch': 1.42}
 36%|███▌      | 150/420 [1:19:41<2:24:18, 32.07s/it] 36%|███▌      | 151/420 [1:20:12<2:23:14, 31.95s/it]                                                     {'loss': 3.3588, 'learning_rate': 1.321867321867322e-05, 'epoch': 1.43}
 36%|███▌      | 151/420 [1:20:12<2:23:14, 31.95s/it] 36%|███▌      | 152/420 [1:20:44<2:22:09, 31.83s/it]                                                     {'loss': 0.8262, 'learning_rate': 1.316953316953317e-05, 'epoch': 1.44}
 36%|███▌      | 152/420 [1:20:44<2:22:09, 31.83s/it] 36%|███▋      | 153/420 [1:21:16<2:21:26, 31.78s/it]                                                     {'loss': 0.8581, 'learning_rate': 1.3120393120393123e-05, 'epoch': 1.45}
 36%|███▋      | 153/420 [1:21:16<2:21:26, 31.78s/it] 37%|███▋      | 154/420 [1:21:47<2:20:37, 31.72s/it]                                                     {'loss': 1.2024, 'learning_rate': 1.3071253071253072e-05, 'epoch': 1.46}
 37%|███▋      | 154/420 [1:21:47<2:20:37, 31.72s/it] 37%|███▋      | 155/420 [1:22:20<2:21:32, 32.05s/it]                                                     {'loss': 0.9354, 'learning_rate': 1.3022113022113022e-05, 'epoch': 1.47}
 37%|███▋      | 155/420 [1:22:20<2:21:32, 32.05s/it] 37%|███▋      | 156/420 [1:22:51<2:19:46, 31.77s/it]                                                     {'loss': 0.9725, 'learning_rate': 1.2972972972972975e-05, 'epoch': 1.48}
 37%|███▋      | 156/420 [1:22:51<2:19:46, 31.77s/it] 37%|███▋      | 157/420 [1:23:23<2:19:11, 31.75s/it]                                                     {'loss': 0.9304, 'learning_rate': 1.2923832923832924e-05, 'epoch': 1.48}
 37%|███▋      | 157/420 [1:23:23<2:19:11, 31.75s/it] 38%|███▊      | 158/420 [1:23:54<2:18:22, 31.69s/it]                                                     {'loss': 0.9949, 'learning_rate': 1.2874692874692876e-05, 'epoch': 1.49}
 38%|███▊      | 158/420 [1:23:54<2:18:22, 31.69s/it] 38%|███▊      | 159/420 [1:24:26<2:17:38, 31.64s/it]                                                     {'loss': 0.9044, 'learning_rate': 1.2825552825552827e-05, 'epoch': 1.5}
 38%|███▊      | 159/420 [1:24:26<2:17:38, 31.64s/it] 38%|███▊      | 160/420 [1:24:57<2:16:22, 31.47s/it]                                                     {'loss': 0.9806, 'learning_rate': 1.2776412776412776e-05, 'epoch': 1.51}
 38%|███▊      | 160/420 [1:24:57<2:16:22, 31.47s/it] 38%|███▊      | 161/420 [1:25:29<2:16:07, 31.54s/it]                                                     {'loss': 1.8015, 'learning_rate': 1.2727272727272728e-05, 'epoch': 1.52}
 38%|███▊      | 161/420 [1:25:29<2:16:07, 31.54s/it] 39%|███▊      | 162/420 [1:26:01<2:16:31, 31.75s/it]                                                     {'loss': 0.8744, 'learning_rate': 1.267813267813268e-05, 'epoch': 1.53}
 39%|███▊      | 162/420 [1:26:01<2:16:31, 31.75s/it] 39%|███▉      | 163/420 [1:26:33<2:16:29, 31.87s/it]                                                     {'loss': 0.8604, 'learning_rate': 1.2628992628992628e-05, 'epoch': 1.54}
 39%|███▉      | 163/420 [1:26:33<2:16:29, 31.87s/it] 39%|███▉      | 164/420 [1:27:05<2:15:27, 31.75s/it]                                                     {'loss': 0.841, 'learning_rate': 1.257985257985258e-05, 'epoch': 1.55}
 39%|███▉      | 164/420 [1:27:05<2:15:27, 31.75s/it] 39%|███▉      | 165/420 [1:27:35<2:13:39, 31.45s/it]                                                     {'loss': 1.0148, 'learning_rate': 1.2530712530712531e-05, 'epoch': 1.56}
 39%|███▉      | 165/420 [1:27:35<2:13:39, 31.45s/it] 40%|███▉      | 166/420 [1:28:07<2:13:14, 31.47s/it]                                                     {'loss': 0.9643, 'learning_rate': 1.2481572481572484e-05, 'epoch': 1.57}
 40%|███▉      | 166/420 [1:28:07<2:13:14, 31.47s/it] 40%|███▉      | 167/420 [1:28:39<2:13:10, 31.58s/it]                                                     {'loss': 0.9687, 'learning_rate': 1.2432432432432433e-05, 'epoch': 1.58}
 40%|███▉      | 167/420 [1:28:39<2:13:10, 31.58s/it] 40%|████      | 168/420 [1:29:10<2:12:44, 31.61s/it]                                                     {'loss': 0.8116, 'learning_rate': 1.2383292383292385e-05, 'epoch': 1.59}
 40%|████      | 168/420 [1:29:10<2:12:44, 31.61s/it] 40%|████      | 169/420 [1:29:42<2:11:57, 31.54s/it]                                                     {'loss': 0.9999, 'learning_rate': 1.2334152334152336e-05, 'epoch': 1.6}
 40%|████      | 169/420 [1:29:42<2:11:57, 31.54s/it] 40%|████      | 170/420 [1:30:13<2:11:15, 31.50s/it]                                                     {'loss': 0.9953, 'learning_rate': 1.2285012285012285e-05, 'epoch': 1.61}
 40%|████      | 170/420 [1:30:13<2:11:15, 31.50s/it] 41%|████      | 171/420 [1:30:45<2:11:08, 31.60s/it]                                                     {'loss': 0.9311, 'learning_rate': 1.2235872235872237e-05, 'epoch': 1.62}
 41%|████      | 171/420 [1:30:45<2:11:08, 31.60s/it] 41%|████      | 172/420 [1:31:17<2:11:45, 31.88s/it]                                                     {'loss': 0.8952, 'learning_rate': 1.2186732186732188e-05, 'epoch': 1.63}
 41%|████      | 172/420 [1:31:17<2:11:45, 31.88s/it] 41%|████      | 173/420 [1:31:50<2:12:10, 32.11s/it]                                                     {'loss': 0.9714, 'learning_rate': 1.2137592137592137e-05, 'epoch': 1.64}
 41%|████      | 173/420 [1:31:50<2:12:10, 32.11s/it] 41%|████▏     | 174/420 [1:32:23<2:12:29, 32.32s/it]                                                     {'loss': 0.9469, 'learning_rate': 1.208845208845209e-05, 'epoch': 1.65}
 41%|████▏     | 174/420 [1:32:23<2:12:29, 32.32s/it] 42%|████▏     | 175/420 [1:32:55<2:11:57, 32.31s/it]                                                     {'loss': 1.863, 'learning_rate': 1.203931203931204e-05, 'epoch': 1.66}
 42%|████▏     | 175/420 [1:32:55<2:11:57, 32.31s/it] 42%|████▏     | 176/420 [1:33:28<2:11:25, 32.32s/it]                                                     {'loss': 1.4941, 'learning_rate': 1.1990171990171991e-05, 'epoch': 1.66}
 42%|████▏     | 176/420 [1:33:28<2:11:25, 32.32s/it] 42%|████▏     | 177/420 [1:34:00<2:10:48, 32.30s/it]                                                     {'loss': 1.0797, 'learning_rate': 1.1941031941031942e-05, 'epoch': 1.67}
 42%|████▏     | 177/420 [1:34:00<2:10:48, 32.30s/it] 42%|████▏     | 178/420 [1:34:31<2:09:20, 32.07s/it]                                                     {'loss': 1.3302, 'learning_rate': 1.1891891891891894e-05, 'epoch': 1.68}
 42%|████▏     | 178/420 [1:34:31<2:09:20, 32.07s/it] 43%|████▎     | 179/420 [1:35:02<2:07:30, 31.74s/it]                                                     {'loss': 1.0563, 'learning_rate': 1.1842751842751843e-05, 'epoch': 1.69}
 43%|████▎     | 179/420 [1:35:02<2:07:30, 31.74s/it] 43%|████▎     | 180/420 [1:35:35<2:07:36, 31.90s/it]                                                     {'loss': 1.4444, 'learning_rate': 1.1793611793611794e-05, 'epoch': 1.7}
 43%|████▎     | 180/420 [1:35:35<2:07:36, 31.90s/it] 43%|████▎     | 181/420 [1:36:07<2:07:58, 32.13s/it]                                                     {'loss': 1.3345, 'learning_rate': 1.1744471744471746e-05, 'epoch': 1.71}
 43%|████▎     | 181/420 [1:36:07<2:07:58, 32.13s/it] 43%|████▎     | 182/420 [1:36:39<2:06:49, 31.97s/it]                                                     {'loss': 1.4413, 'learning_rate': 1.1695331695331696e-05, 'epoch': 1.72}
 43%|████▎     | 182/420 [1:36:39<2:06:49, 31.97s/it] 44%|████▎     | 183/420 [1:37:11<2:06:20, 31.99s/it]                                                     {'loss': 0.9694, 'learning_rate': 1.1646191646191646e-05, 'epoch': 1.73}
 44%|████▎     | 183/420 [1:37:11<2:06:20, 31.99s/it] 44%|████▍     | 184/420 [1:37:43<2:05:55, 32.01s/it]                                                     {'loss': 1.4859, 'learning_rate': 1.1597051597051599e-05, 'epoch': 1.74}
 44%|████▍     | 184/420 [1:37:43<2:05:55, 32.01s/it] 44%|████▍     | 185/420 [1:38:15<2:05:18, 31.99s/it]                                                     {'loss': 0.9117, 'learning_rate': 1.1547911547911548e-05, 'epoch': 1.75}
 44%|████▍     | 185/420 [1:38:15<2:05:18, 31.99s/it] 44%|████▍     | 186/420 [1:38:47<2:05:22, 32.15s/it]                                                     {'loss': 0.9698, 'learning_rate': 1.14987714987715e-05, 'epoch': 1.76}
 44%|████▍     | 186/420 [1:38:47<2:05:22, 32.15s/it] 45%|████▍     | 187/420 [1:39:18<2:03:13, 31.73s/it]                                                     {'loss': 1.0277, 'learning_rate': 1.1449631449631451e-05, 'epoch': 1.77}
 45%|████▍     | 187/420 [1:39:18<2:03:13, 31.73s/it] 45%|████▍     | 188/420 [1:39:49<2:02:02, 31.56s/it]                                                     {'loss': 0.9427, 'learning_rate': 1.1400491400491403e-05, 'epoch': 1.78}
 45%|████▍     | 188/420 [1:39:49<2:02:02, 31.56s/it] 45%|████▌     | 189/420 [1:40:21<2:01:05, 31.45s/it]                                                     {'loss': 0.894, 'learning_rate': 1.1351351351351352e-05, 'epoch': 1.79}
 45%|████▌     | 189/420 [1:40:21<2:01:05, 31.45s/it] 45%|████▌     | 190/420 [1:40:53<2:01:17, 31.64s/it]                                                     {'loss': 0.8995, 'learning_rate': 1.1302211302211303e-05, 'epoch': 1.8}
 45%|████▌     | 190/420 [1:40:53<2:01:17, 31.64s/it] 45%|████▌     | 191/420 [1:41:25<2:01:14, 31.77s/it]                                                     {'loss': 0.9266, 'learning_rate': 1.1253071253071255e-05, 'epoch': 1.81}
 45%|████▌     | 191/420 [1:41:25<2:01:14, 31.77s/it] 46%|████▌     | 192/420 [1:41:56<1:59:58, 31.57s/it]                                                     {'loss': 0.8882, 'learning_rate': 1.1203931203931205e-05, 'epoch': 1.82}
 46%|████▌     | 192/420 [1:41:56<1:59:58, 31.57s/it] 46%|████▌     | 193/420 [1:42:27<1:59:10, 31.50s/it]                                                     {'loss': 0.9729, 'learning_rate': 1.1154791154791155e-05, 'epoch': 1.83}
 46%|████▌     | 193/420 [1:42:27<1:59:10, 31.50s/it] 46%|████▌     | 194/420 [1:43:00<1:59:51, 31.82s/it]                                                     {'loss': 1.7359, 'learning_rate': 1.1105651105651108e-05, 'epoch': 1.83}
 46%|████▌     | 194/420 [1:43:00<1:59:51, 31.82s/it] 46%|████▋     | 195/420 [1:43:31<1:59:12, 31.79s/it]                                                     {'loss': 1.0007, 'learning_rate': 1.1056511056511057e-05, 'epoch': 1.84}
 46%|████▋     | 195/420 [1:43:31<1:59:12, 31.79s/it] 47%|████▋     | 196/420 [1:44:03<1:58:43, 31.80s/it]                                                     {'loss': 0.9124, 'learning_rate': 1.1007371007371007e-05, 'epoch': 1.85}
 47%|████▋     | 196/420 [1:44:03<1:58:43, 31.80s/it] 47%|████▋     | 197/420 [1:44:35<1:58:14, 31.81s/it]                                                     {'loss': 0.9365, 'learning_rate': 1.095823095823096e-05, 'epoch': 1.86}
 47%|████▋     | 197/420 [1:44:35<1:58:14, 31.81s/it] 47%|████▋     | 198/420 [1:45:07<1:57:24, 31.73s/it]                                                     {'loss': 1.5759, 'learning_rate': 1.0909090909090909e-05, 'epoch': 1.87}
 47%|████▋     | 198/420 [1:45:07<1:57:24, 31.73s/it] 47%|████▋     | 199/420 [1:45:39<1:57:29, 31.90s/it]                                                     {'loss': 1.356, 'learning_rate': 1.0859950859950861e-05, 'epoch': 1.88}
 47%|████▋     | 199/420 [1:45:39<1:57:29, 31.90s/it] 48%|████▊     | 200/420 [1:46:10<1:56:03, 31.65s/it]                                                     {'loss': 0.9207, 'learning_rate': 1.0810810810810812e-05, 'epoch': 1.89}
 48%|████▊     | 200/420 [1:46:10<1:56:03, 31.65s/it] 48%|████▊     | 201/420 [1:46:42<1:55:30, 31.65s/it]                                                     {'loss': 0.9304, 'learning_rate': 1.0761670761670761e-05, 'epoch': 1.9}
 48%|████▊     | 201/420 [1:46:42<1:55:30, 31.65s/it] 48%|████▊     | 202/420 [1:47:14<1:55:33, 31.81s/it]                                                     {'loss': 1.3074, 'learning_rate': 1.0712530712530714e-05, 'epoch': 1.91}
 48%|████▊     | 202/420 [1:47:14<1:55:33, 31.81s/it] 48%|████▊     | 203/420 [1:47:45<1:54:43, 31.72s/it]                                                     {'loss': 0.7729, 'learning_rate': 1.0663390663390664e-05, 'epoch': 1.92}
 48%|████▊     | 203/420 [1:47:45<1:54:43, 31.72s/it] 49%|████▊     | 204/420 [1:48:16<1:53:22, 31.49s/it]                                                     {'loss': 1.0011, 'learning_rate': 1.0614250614250613e-05, 'epoch': 1.93}
 49%|████▊     | 204/420 [1:48:16<1:53:22, 31.49s/it] 49%|████▉     | 205/420 [1:48:49<1:53:52, 31.78s/it]                                                     {'loss': 0.9778, 'learning_rate': 1.0565110565110566e-05, 'epoch': 1.94}
 49%|████▉     | 205/420 [1:48:49<1:53:52, 31.78s/it] 49%|████▉     | 206/420 [1:49:20<1:53:07, 31.72s/it]                                                     {'loss': 0.835, 'learning_rate': 1.0515970515970516e-05, 'epoch': 1.95}
 49%|████▉     | 206/420 [1:49:20<1:53:07, 31.72s/it] 49%|████▉     | 207/420 [1:49:52<1:52:46, 31.77s/it]                                                     {'loss': 1.0618, 'learning_rate': 1.0466830466830469e-05, 'epoch': 1.96}
 49%|████▉     | 207/420 [1:49:52<1:52:46, 31.77s/it] 50%|████▉     | 208/420 [1:50:23<1:51:44, 31.62s/it]                                                     {'loss': 0.9698, 'learning_rate': 1.0417690417690418e-05, 'epoch': 1.97}
 50%|████▉     | 208/420 [1:50:23<1:51:44, 31.62s/it] 50%|████▉     | 209/420 [1:50:56<1:51:59, 31.84s/it]                                                     {'loss': 0.947, 'learning_rate': 1.036855036855037e-05, 'epoch': 1.98}
 50%|████▉     | 209/420 [1:50:56<1:51:59, 31.84s/it] 50%|█████     | 210/420 [1:51:28<1:51:15, 31.79s/it]                                                     {'loss': 0.8436, 'learning_rate': 1.0319410319410321e-05, 'epoch': 1.99}
 50%|█████     | 210/420 [1:51:28<1:51:15, 31.79s/it] 50%|█████     | 211/420 [1:52:00<1:51:31, 32.02s/it]                                                     {'loss': 0.9254, 'learning_rate': 1.027027027027027e-05, 'epoch': 2.0}
 50%|█████     | 211/420 [1:52:00<1:51:31, 32.02s/it][INFO|trainer.py:2889] 2024-07-01 03:20:30,777 >> Saving model checkpoint to ../out/Llama-2-7b-p0.05-lora-seed3-bsz128/tmp-checkpoint-211
[INFO|tokenization_utils_base.py:2432] 2024-07-01 03:20:32,151 >> tokenizer config file saved in ../out/Llama-2-7b-p0.05-lora-seed3-bsz128/tmp-checkpoint-211/tokenizer_config.json
[INFO|tokenization_utils_base.py:2441] 2024-07-01 03:20:32,152 >> Special tokens file saved in ../out/Llama-2-7b-p0.05-lora-seed3-bsz128/tmp-checkpoint-211/special_tokens_map.json
 50%|█████     | 212/420 [1:53:41<3:02:18, 52.59s/it]                                                     {'loss': 0.9047, 'learning_rate': 1.0221130221130223e-05, 'epoch': 2.01}
 50%|█████     | 212/420 [1:53:41<3:02:18, 52.59s/it] 51%|█████     | 213/420 [1:54:12<2:39:53, 46.34s/it]                                                     {'loss': 0.8949, 'learning_rate': 1.0171990171990173e-05, 'epoch': 2.01}
 51%|█████     | 213/420 [1:54:12<2:39:53, 46.34s/it] 51%|█████     | 214/420 [1:54:45<2:24:38, 42.13s/it]                                                     {'loss': 0.8955, 'learning_rate': 1.0122850122850122e-05, 'epoch': 2.02}
 51%|█████     | 214/420 [1:54:45<2:24:38, 42.13s/it] 51%|█████     | 215/420 [1:55:17<2:14:21, 39.32s/it]                                                     {'loss': 2.3778, 'learning_rate': 1.0073710073710075e-05, 'epoch': 2.03}
 51%|█████     | 215/420 [1:55:17<2:14:21, 39.32s/it] 51%|█████▏    | 216/420 [1:55:50<2:06:15, 37.13s/it]                                                     {'loss': 0.8686, 'learning_rate': 1.0024570024570025e-05, 'epoch': 2.04}
 51%|█████▏    | 216/420 [1:55:50<2:06:15, 37.13s/it] 52%|█████▏    | 217/420 [1:56:21<2:00:16, 35.55s/it]                                                     {'loss': 1.6378, 'learning_rate': 9.975429975429976e-06, 'epoch': 2.05}
 52%|█████▏    | 217/420 [1:56:21<2:00:16, 35.55s/it] 52%|█████▏    | 218/420 [1:56:52<1:55:00, 34.16s/it]                                                     {'loss': 0.8772, 'learning_rate': 9.926289926289927e-06, 'epoch': 2.06}
 52%|█████▏    | 218/420 [1:56:52<1:55:00, 34.16s/it] 52%|█████▏    | 219/420 [1:57:24<1:51:51, 33.39s/it]                                                     {'loss': 1.2574, 'learning_rate': 9.877149877149878e-06, 'epoch': 2.07}
 52%|█████▏    | 219/420 [1:57:24<1:51:51, 33.39s/it] 52%|█████▏    | 220/420 [1:57:55<1:49:27, 32.84s/it]                                                     {'loss': 0.8377, 'learning_rate': 9.828009828009828e-06, 'epoch': 2.08}
 52%|█████▏    | 220/420 [1:57:55<1:49:27, 32.84s/it] 53%|█████▎    | 221/420 [1:58:27<1:47:52, 32.53s/it]                                                     {'loss': 1.079, 'learning_rate': 9.778869778869779e-06, 'epoch': 2.09}
 53%|█████▎    | 221/420 [1:58:27<1:47:52, 32.53s/it] 53%|█████▎    | 222/420 [1:58:58<1:45:20, 31.92s/it]                                                     {'loss': 0.9086, 'learning_rate': 9.729729729729732e-06, 'epoch': 2.1}
 53%|█████▎    | 222/420 [1:58:58<1:45:20, 31.92s/it] 53%|█████▎    | 223/420 [1:59:31<1:45:39, 32.18s/it]                                                     {'loss': 0.8704, 'learning_rate': 9.680589680589682e-06, 'epoch': 2.11}
 53%|█████▎    | 223/420 [1:59:31<1:45:39, 32.18s/it] 53%|█████▎    | 224/420 [2:00:03<1:45:12, 32.20s/it]                                                     {'loss': 0.8068, 'learning_rate': 9.631449631449631e-06, 'epoch': 2.12}
 53%|█████▎    | 224/420 [2:00:03<1:45:12, 32.20s/it] 54%|█████▎    | 225/420 [2:00:34<1:43:59, 32.00s/it]                                                     {'loss': 0.8199, 'learning_rate': 9.582309582309584e-06, 'epoch': 2.13}
 54%|█████▎    | 225/420 [2:00:34<1:43:59, 32.00s/it] 54%|█████▍    | 226/420 [2:01:05<1:42:36, 31.74s/it]                                                     {'loss': 0.8604, 'learning_rate': 9.533169533169534e-06, 'epoch': 2.14}
 54%|█████▍    | 226/420 [2:01:05<1:42:36, 31.74s/it] 54%|█████▍    | 227/420 [2:01:37<1:42:00, 31.71s/it]                                                     {'loss': 0.961, 'learning_rate': 9.484029484029485e-06, 'epoch': 2.15}
 54%|█████▍    | 227/420 [2:01:37<1:42:00, 31.71s/it] 54%|█████▍    | 228/420 [2:02:10<1:42:43, 32.10s/it]                                                     {'loss': 1.4205, 'learning_rate': 9.434889434889436e-06, 'epoch': 2.16}
 54%|█████▍    | 228/420 [2:02:10<1:42:43, 32.10s/it] 55%|█████▍    | 229/420 [2:02:42<1:41:52, 32.00s/it]                                                     {'loss': 0.8447, 'learning_rate': 9.385749385749387e-06, 'epoch': 2.17}
 55%|█████▍    | 229/420 [2:02:42<1:41:52, 32.00s/it] 55%|█████▍    | 230/420 [2:03:14<1:41:03, 31.91s/it]                                                     {'loss': 0.9673, 'learning_rate': 9.336609336609337e-06, 'epoch': 2.18}
 55%|█████▍    | 230/420 [2:03:14<1:41:03, 31.91s/it] 55%|█████▌    | 231/420 [2:03:46<1:41:05, 32.09s/it]                                                     {'loss': 0.6804, 'learning_rate': 9.287469287469288e-06, 'epoch': 2.18}
 55%|█████▌    | 231/420 [2:03:46<1:41:05, 32.09s/it] 55%|█████▌    | 232/420 [2:04:17<1:38:59, 31.59s/it]                                                     {'loss': 0.9037, 'learning_rate': 9.238329238329239e-06, 'epoch': 2.19}
 55%|█████▌    | 232/420 [2:04:17<1:38:59, 31.59s/it] 55%|█████▌    | 233/420 [2:04:48<1:38:47, 31.70s/it]                                                     {'loss': 1.0538, 'learning_rate': 9.189189189189191e-06, 'epoch': 2.2}
 55%|█████▌    | 233/420 [2:04:48<1:38:47, 31.70s/it] 56%|█████▌    | 234/420 [2:05:21<1:38:51, 31.89s/it]                                                     {'loss': 1.0561, 'learning_rate': 9.14004914004914e-06, 'epoch': 2.21}
 56%|█████▌    | 234/420 [2:05:21<1:38:51, 31.89s/it] 56%|█████▌    | 235/420 [2:05:53<1:38:35, 31.98s/it]                                                     {'loss': 0.9252, 'learning_rate': 9.090909090909091e-06, 'epoch': 2.22}
 56%|█████▌    | 235/420 [2:05:53<1:38:35, 31.98s/it] 56%|█████▌    | 236/420 [2:06:24<1:37:29, 31.79s/it]                                                     {'loss': 0.8424, 'learning_rate': 9.041769041769043e-06, 'epoch': 2.23}
 56%|█████▌    | 236/420 [2:06:24<1:37:29, 31.79s/it] 56%|█████▋    | 237/420 [2:06:57<1:37:22, 31.92s/it]                                                     {'loss': 0.8841, 'learning_rate': 8.992628992628994e-06, 'epoch': 2.24}
 56%|█████▋    | 237/420 [2:06:57<1:37:22, 31.92s/it] 57%|█████▋    | 238/420 [2:07:28<1:36:29, 31.81s/it]                                                     {'loss': 0.8986, 'learning_rate': 8.943488943488943e-06, 'epoch': 2.25}
 57%|█████▋    | 238/420 [2:07:28<1:36:29, 31.81s/it] 57%|█████▋    | 239/420 [2:08:01<1:36:49, 32.09s/it]                                                     {'loss': 0.822, 'learning_rate': 8.894348894348896e-06, 'epoch': 2.26}
 57%|█████▋    | 239/420 [2:08:01<1:36:49, 32.09s/it] 57%|█████▋    | 240/420 [2:08:34<1:36:52, 32.29s/it]                                                     {'loss': 0.867, 'learning_rate': 8.845208845208846e-06, 'epoch': 2.27}
 57%|█████▋    | 240/420 [2:08:34<1:36:52, 32.29s/it] 57%|█████▋    | 241/420 [2:09:05<1:35:43, 32.09s/it]                                                     {'loss': 0.7685, 'learning_rate': 8.796068796068795e-06, 'epoch': 2.28}
 57%|█████▋    | 241/420 [2:09:05<1:35:43, 32.09s/it] 58%|█████▊    | 242/420 [2:09:36<1:34:11, 31.75s/it]                                                     {'loss': 0.8888, 'learning_rate': 8.746928746928748e-06, 'epoch': 2.29}
 58%|█████▊    | 242/420 [2:09:36<1:34:11, 31.75s/it] 58%|█████▊    | 243/420 [2:10:08<1:33:25, 31.67s/it]                                                     {'loss': 0.89, 'learning_rate': 8.697788697788699e-06, 'epoch': 2.3}
 58%|█████▊    | 243/420 [2:10:08<1:33:25, 31.67s/it] 58%|█████▊    | 244/420 [2:10:39<1:33:01, 31.71s/it]                                                     {'loss': 0.8215, 'learning_rate': 8.64864864864865e-06, 'epoch': 2.31}
 58%|█████▊    | 244/420 [2:10:39<1:33:01, 31.71s/it] 58%|█████▊    | 245/420 [2:11:12<1:33:28, 32.05s/it]                                                     {'loss': 1.1385, 'learning_rate': 8.5995085995086e-06, 'epoch': 2.32}
 58%|█████▊    | 245/420 [2:11:12<1:33:28, 32.05s/it] 59%|█████▊    | 246/420 [2:11:44<1:32:44, 31.98s/it]                                                     {'loss': 1.9423, 'learning_rate': 8.55036855036855e-06, 'epoch': 2.33}
 59%|█████▊    | 246/420 [2:11:44<1:32:44, 31.98s/it] 59%|█████▉    | 247/420 [2:12:16<1:31:52, 31.86s/it]                                                     {'loss': 0.8408, 'learning_rate': 8.501228501228502e-06, 'epoch': 2.34}
 59%|█████▉    | 247/420 [2:12:16<1:31:52, 31.86s/it] 59%|█████▉    | 248/420 [2:12:47<1:30:33, 31.59s/it]                                                     {'loss': 0.9406, 'learning_rate': 8.452088452088452e-06, 'epoch': 2.35}
 59%|█████▉    | 248/420 [2:12:47<1:30:33, 31.59s/it] 59%|█████▉    | 249/420 [2:13:19<1:30:27, 31.74s/it]                                                     {'loss': 1.642, 'learning_rate': 8.402948402948403e-06, 'epoch': 2.36}
 59%|█████▉    | 249/420 [2:13:19<1:30:27, 31.74s/it] 60%|█████▉    | 250/420 [2:13:51<1:30:09, 31.82s/it]                                                     {'loss': 1.4179, 'learning_rate': 8.353808353808355e-06, 'epoch': 2.36}
 60%|█████▉    | 250/420 [2:13:51<1:30:09, 31.82s/it] 60%|█████▉    | 251/420 [2:14:23<1:30:11, 32.02s/it]                                                     {'loss': 0.7772, 'learning_rate': 8.304668304668304e-06, 'epoch': 2.37}
 60%|█████▉    | 251/420 [2:14:23<1:30:11, 32.02s/it] 60%|██████    | 252/420 [2:14:56<1:30:02, 32.16s/it]                                                     {'loss': 1.2362, 'learning_rate': 8.255528255528255e-06, 'epoch': 2.38}
 60%|██████    | 252/420 [2:14:56<1:30:02, 32.16s/it] 60%|██████    | 253/420 [2:15:27<1:28:49, 31.91s/it]                                                     {'loss': 0.7949, 'learning_rate': 8.206388206388208e-06, 'epoch': 2.39}
 60%|██████    | 253/420 [2:15:27<1:28:49, 31.91s/it] 60%|██████    | 254/420 [2:15:59<1:28:20, 31.93s/it]                                                     {'loss': 0.8222, 'learning_rate': 8.157248157248158e-06, 'epoch': 2.4}
 60%|██████    | 254/420 [2:15:59<1:28:20, 31.93s/it] 61%|██████    | 255/420 [2:16:30<1:27:18, 31.75s/it]                                                     {'loss': 1.3128, 'learning_rate': 8.108108108108109e-06, 'epoch': 2.41}
 61%|██████    | 255/420 [2:16:30<1:27:18, 31.75s/it] 61%|██████    | 256/420 [2:17:03<1:27:20, 31.95s/it]                                                     {'loss': 0.9001, 'learning_rate': 8.05896805896806e-06, 'epoch': 2.42}
 61%|██████    | 256/420 [2:17:03<1:27:20, 31.95s/it] 61%|██████    | 257/420 [2:17:34<1:26:17, 31.76s/it]                                                     {'loss': 0.8725, 'learning_rate': 8.00982800982801e-06, 'epoch': 2.43}
 61%|██████    | 257/420 [2:17:34<1:26:17, 31.76s/it] 61%|██████▏   | 258/420 [2:18:06<1:25:30, 31.67s/it]                                                     {'loss': 0.8516, 'learning_rate': 7.960687960687961e-06, 'epoch': 2.44}
 61%|██████▏   | 258/420 [2:18:06<1:25:30, 31.67s/it] 62%|██████▏   | 259/420 [2:18:37<1:24:22, 31.44s/it]                                                     {'loss': 0.9845, 'learning_rate': 7.911547911547912e-06, 'epoch': 2.45}
 62%|██████▏   | 259/420 [2:18:37<1:24:22, 31.44s/it] 62%|██████▏   | 260/420 [2:19:08<1:23:43, 31.40s/it]                                                     {'loss': 0.8411, 'learning_rate': 7.862407862407863e-06, 'epoch': 2.46}
 62%|██████▏   | 260/420 [2:19:08<1:23:43, 31.40s/it] 62%|██████▏   | 261/420 [2:19:40<1:23:34, 31.53s/it]                                                     {'loss': 1.0725, 'learning_rate': 7.813267813267813e-06, 'epoch': 2.47}
 62%|██████▏   | 261/420 [2:19:40<1:23:34, 31.53s/it] 62%|██████▏   | 262/420 [2:20:11<1:22:31, 31.34s/it]                                                     {'loss': 0.959, 'learning_rate': 7.764127764127764e-06, 'epoch': 2.48}
 62%|██████▏   | 262/420 [2:20:11<1:22:31, 31.34s/it] 63%|██████▎   | 263/420 [2:20:42<1:22:11, 31.41s/it]                                                     {'loss': 0.9125, 'learning_rate': 7.714987714987717e-06, 'epoch': 2.49}
 63%|██████▎   | 263/420 [2:20:42<1:22:11, 31.41s/it] 63%|██████▎   | 264/420 [2:21:14<1:21:52, 31.49s/it]                                                     {'loss': 0.8839, 'learning_rate': 7.665847665847667e-06, 'epoch': 2.5}
 63%|██████▎   | 264/420 [2:21:14<1:21:52, 31.49s/it] 63%|██████▎   | 265/420 [2:21:46<1:21:35, 31.58s/it]                                                     {'loss': 0.8582, 'learning_rate': 7.616707616707617e-06, 'epoch': 2.51}
 63%|██████▎   | 265/420 [2:21:46<1:21:35, 31.58s/it] 63%|██████▎   | 266/420 [2:22:18<1:21:20, 31.69s/it]                                                     {'loss': 1.0918, 'learning_rate': 7.567567567567569e-06, 'epoch': 2.52}
 63%|██████▎   | 266/420 [2:22:18<1:21:20, 31.69s/it] 64%|██████▎   | 267/420 [2:22:49<1:20:34, 31.60s/it]                                                     {'loss': 1.6876, 'learning_rate': 7.518427518427519e-06, 'epoch': 2.53}
 64%|██████▎   | 267/420 [2:22:49<1:20:34, 31.60s/it] 64%|██████▍   | 268/420 [2:23:20<1:20:00, 31.58s/it]                                                     {'loss': 0.7787, 'learning_rate': 7.469287469287469e-06, 'epoch': 2.53}
 64%|██████▍   | 268/420 [2:23:20<1:20:00, 31.58s/it] 64%|██████▍   | 269/420 [2:23:52<1:19:31, 31.60s/it]                                                     {'loss': 0.8288, 'learning_rate': 7.420147420147421e-06, 'epoch': 2.54}
 64%|██████▍   | 269/420 [2:23:52<1:19:31, 31.60s/it] 64%|██████▍   | 270/420 [2:24:23<1:18:36, 31.44s/it]                                                     {'loss': 0.9634, 'learning_rate': 7.371007371007372e-06, 'epoch': 2.55}
 64%|██████▍   | 270/420 [2:24:23<1:18:36, 31.44s/it] 65%|██████▍   | 271/420 [2:24:54<1:17:56, 31.39s/it]                                                     {'loss': 1.2137, 'learning_rate': 7.321867321867322e-06, 'epoch': 2.56}
 65%|██████▍   | 271/420 [2:24:54<1:17:56, 31.39s/it] 65%|██████▍   | 272/420 [2:25:26<1:17:39, 31.48s/it]                                                     {'loss': 2.585, 'learning_rate': 7.272727272727273e-06, 'epoch': 2.57}
 65%|██████▍   | 272/420 [2:25:26<1:17:39, 31.48s/it] 65%|██████▌   | 273/420 [2:25:57<1:16:58, 31.42s/it]                                                     {'loss': 1.014, 'learning_rate': 7.223587223587224e-06, 'epoch': 2.58}
 65%|██████▌   | 273/420 [2:25:57<1:16:58, 31.42s/it] 65%|██████▌   | 274/420 [2:26:28<1:15:35, 31.07s/it]                                                     {'loss': 0.8623, 'learning_rate': 7.1744471744471755e-06, 'epoch': 2.59}
 65%|██████▌   | 274/420 [2:26:28<1:15:35, 31.07s/it] 65%|██████▌   | 275/420 [2:26:59<1:14:58, 31.02s/it]                                                     {'loss': 0.9615, 'learning_rate': 7.125307125307126e-06, 'epoch': 2.6}
 65%|██████▌   | 275/420 [2:26:59<1:14:58, 31.02s/it] 66%|██████▌   | 276/420 [2:27:30<1:14:50, 31.18s/it]                                                     {'loss': 0.9866, 'learning_rate': 7.076167076167076e-06, 'epoch': 2.61}
 66%|██████▌   | 276/420 [2:27:30<1:14:50, 31.18s/it] 66%|██████▌   | 277/420 [2:28:02<1:14:37, 31.31s/it]                                                     {'loss': 0.9013, 'learning_rate': 7.027027027027028e-06, 'epoch': 2.62}
 66%|██████▌   | 277/420 [2:28:02<1:14:37, 31.31s/it] 66%|██████▌   | 278/420 [2:28:32<1:13:34, 31.09s/it]                                                     {'loss': 0.9644, 'learning_rate': 6.977886977886978e-06, 'epoch': 2.63}
 66%|██████▌   | 278/420 [2:28:32<1:13:34, 31.09s/it] 66%|██████▋   | 279/420 [2:29:02<1:12:24, 30.81s/it]                                                     {'loss': 0.7795, 'learning_rate': 6.928746928746929e-06, 'epoch': 2.64}
 66%|██████▋   | 279/420 [2:29:02<1:12:24, 30.81s/it] 67%|██████▋   | 280/420 [2:29:33<1:11:21, 30.58s/it]                                                     {'loss': 0.8392, 'learning_rate': 6.879606879606881e-06, 'epoch': 2.65}
 67%|██████▋   | 280/420 [2:29:33<1:11:21, 30.58s/it] 67%|██████▋   | 281/420 [2:30:03<1:10:49, 30.57s/it]                                                     {'loss': 0.8856, 'learning_rate': 6.830466830466831e-06, 'epoch': 2.66}
 67%|██████▋   | 281/420 [2:30:03<1:10:49, 30.57s/it] 67%|██████▋   | 282/420 [2:30:34<1:10:52, 30.82s/it]                                                     {'loss': 0.853, 'learning_rate': 6.781326781326781e-06, 'epoch': 2.67}
 67%|██████▋   | 282/420 [2:30:34<1:10:52, 30.82s/it] 67%|██████▋   | 283/420 [2:31:06<1:10:53, 31.05s/it]                                                     {'loss': 0.9689, 'learning_rate': 6.732186732186733e-06, 'epoch': 2.68}
 67%|██████▋   | 283/420 [2:31:06<1:10:53, 31.05s/it] 68%|██████▊   | 284/420 [2:31:36<1:09:43, 30.76s/it]                                                     {'loss': 0.7918, 'learning_rate': 6.683046683046684e-06, 'epoch': 2.69}
 68%|██████▊   | 284/420 [2:31:36<1:09:43, 30.76s/it] 68%|██████▊   | 285/420 [2:32:07<1:09:08, 30.73s/it]                                                     {'loss': 1.3316, 'learning_rate': 6.633906633906635e-06, 'epoch': 2.7}
 68%|██████▊   | 285/420 [2:32:07<1:09:08, 30.73s/it] 68%|██████▊   | 286/420 [2:32:37<1:08:14, 30.55s/it]                                                     {'loss': 0.862, 'learning_rate': 6.584766584766585e-06, 'epoch': 2.71}
 68%|██████▊   | 286/420 [2:32:37<1:08:14, 30.55s/it] 68%|██████▊   | 287/420 [2:33:08<1:08:21, 30.84s/it]                                                     {'loss': 1.4917, 'learning_rate': 6.535626535626536e-06, 'epoch': 2.71}
 68%|██████▊   | 287/420 [2:33:08<1:08:21, 30.84s/it] 69%|██████▊   | 288/420 [2:33:39<1:07:37, 30.74s/it]                                                     {'loss': 0.9172, 'learning_rate': 6.486486486486487e-06, 'epoch': 2.72}
 69%|██████▊   | 288/420 [2:33:39<1:07:37, 30.74s/it] 69%|██████▉   | 289/420 [2:34:10<1:07:32, 30.93s/it]                                                     {'loss': 0.909, 'learning_rate': 6.437346437346438e-06, 'epoch': 2.73}
 69%|██████▉   | 289/420 [2:34:10<1:07:32, 30.93s/it] 69%|██████▉   | 290/420 [2:34:41<1:07:02, 30.94s/it]                                                     {'loss': 1.3671, 'learning_rate': 6.388206388206388e-06, 'epoch': 2.74}
 69%|██████▉   | 290/420 [2:34:41<1:07:02, 30.94s/it] 69%|██████▉   | 291/420 [2:35:12<1:06:06, 30.75s/it]                                                     {'loss': 0.9094, 'learning_rate': 6.33906633906634e-06, 'epoch': 2.75}
 69%|██████▉   | 291/420 [2:35:12<1:06:06, 30.75s/it] 70%|██████▉   | 292/420 [2:35:42<1:05:37, 30.76s/it]                                                     {'loss': 0.8533, 'learning_rate': 6.28992628992629e-06, 'epoch': 2.76}
 70%|██████▉   | 292/420 [2:35:42<1:05:37, 30.76s/it] 70%|██████▉   | 293/420 [2:36:14<1:05:23, 30.89s/it]                                                     {'loss': 3.0315, 'learning_rate': 6.240786240786242e-06, 'epoch': 2.77}
 70%|██████▉   | 293/420 [2:36:14<1:05:23, 30.89s/it] 70%|███████   | 294/420 [2:36:44<1:04:50, 30.87s/it]                                                     {'loss': 0.8581, 'learning_rate': 6.191646191646193e-06, 'epoch': 2.78}
 70%|███████   | 294/420 [2:36:44<1:04:50, 30.87s/it] 70%|███████   | 295/420 [2:37:16<1:04:41, 31.05s/it]                                                     {'loss': 0.9933, 'learning_rate': 6.1425061425061425e-06, 'epoch': 2.79}
 70%|███████   | 295/420 [2:37:16<1:04:41, 31.05s/it] 70%|███████   | 296/420 [2:37:46<1:03:43, 30.83s/it]                                                     {'loss': 0.9021, 'learning_rate': 6.093366093366094e-06, 'epoch': 2.8}
 70%|███████   | 296/420 [2:37:46<1:03:43, 30.83s/it] 71%|███████   | 297/420 [2:38:17<1:02:57, 30.71s/it]                                                     {'loss': 0.8802, 'learning_rate': 6.044226044226045e-06, 'epoch': 2.81}
 71%|███████   | 297/420 [2:38:17<1:02:57, 30.71s/it] 71%|███████   | 298/420 [2:38:47<1:02:12, 30.59s/it]                                                     {'loss': 0.8406, 'learning_rate': 5.9950859950859956e-06, 'epoch': 2.82}
 71%|███████   | 298/420 [2:38:47<1:02:12, 30.59s/it] 71%|███████   | 299/420 [2:39:17<1:01:39, 30.58s/it]                                                     {'loss': 0.9119, 'learning_rate': 5.945945945945947e-06, 'epoch': 2.83}
 71%|███████   | 299/420 [2:39:17<1:01:39, 30.58s/it] 71%|███████▏  | 300/420 [2:39:49<1:01:44, 30.87s/it]                                                     {'loss': 0.8548, 'learning_rate': 5.896805896805897e-06, 'epoch': 2.84}
 71%|███████▏  | 300/420 [2:39:49<1:01:44, 30.87s/it] 72%|███████▏  | 301/420 [2:40:20<1:01:05, 30.80s/it]                                                     {'loss': 0.9148, 'learning_rate': 5.847665847665848e-06, 'epoch': 2.85}
 72%|███████▏  | 301/420 [2:40:20<1:01:05, 30.80s/it] 72%|███████▏  | 302/420 [2:40:51<1:00:43, 30.88s/it]                                                     {'loss': 0.9174, 'learning_rate': 5.798525798525799e-06, 'epoch': 2.86}
 72%|███████▏  | 302/420 [2:40:51<1:00:43, 30.88s/it] 72%|███████▏  | 303/420 [2:41:22<1:00:09, 30.85s/it]                                                     {'loss': 0.7619, 'learning_rate': 5.74938574938575e-06, 'epoch': 2.87}
 72%|███████▏  | 303/420 [2:41:22<1:00:09, 30.85s/it] 72%|███████▏  | 304/420 [2:41:52<59:26, 30.75s/it]                                                     {'loss': 1.6861, 'learning_rate': 5.700245700245702e-06, 'epoch': 2.88}
 72%|███████▏  | 304/420 [2:41:52<59:26, 30.75s/it] 73%|███████▎  | 305/420 [2:42:22<58:43, 30.64s/it]                                                   {'loss': 0.9149, 'learning_rate': 5.6511056511056515e-06, 'epoch': 2.88}
 73%|███████▎  | 305/420 [2:42:22<58:43, 30.64s/it] 73%|███████▎  | 306/420 [2:42:53<58:25, 30.75s/it]                                                   {'loss': 0.8834, 'learning_rate': 5.601965601965602e-06, 'epoch': 2.89}
 73%|███████▎  | 306/420 [2:42:53<58:25, 30.75s/it] 73%|███████▎  | 307/420 [2:43:24<57:59, 30.79s/it]                                                   {'loss': 0.8321, 'learning_rate': 5.552825552825554e-06, 'epoch': 2.9}
 73%|███████▎  | 307/420 [2:43:24<57:59, 30.79s/it] 73%|███████▎  | 308/420 [2:43:56<57:42, 30.92s/it]                                                   {'loss': 0.8686, 'learning_rate': 5.503685503685504e-06, 'epoch': 2.91}
 73%|███████▎  | 308/420 [2:43:56<57:42, 30.92s/it] 74%|███████▎  | 309/420 [2:44:26<57:09, 30.90s/it]                                                   {'loss': 1.0013, 'learning_rate': 5.4545454545454545e-06, 'epoch': 2.92}
 74%|███████▎  | 309/420 [2:44:26<57:09, 30.90s/it] 74%|███████▍  | 310/420 [2:44:57<56:19, 30.72s/it]                                                   {'loss': 0.9176, 'learning_rate': 5.405405405405406e-06, 'epoch': 2.93}
 74%|███████▍  | 310/420 [2:44:57<56:19, 30.72s/it] 74%|███████▍  | 311/420 [2:45:27<55:40, 30.65s/it]                                                   {'loss': 1.0309, 'learning_rate': 5.356265356265357e-06, 'epoch': 2.94}
 74%|███████▍  | 311/420 [2:45:27<55:40, 30.65s/it] 74%|███████▍  | 312/420 [2:45:58<55:09, 30.65s/it]                                                   {'loss': 0.9934, 'learning_rate': 5.307125307125307e-06, 'epoch': 2.95}
 74%|███████▍  | 312/420 [2:45:58<55:09, 30.65s/it] 75%|███████▍  | 313/420 [2:46:28<54:32, 30.58s/it]                                                   {'loss': 0.745, 'learning_rate': 5.257985257985258e-06, 'epoch': 2.96}
 75%|███████▍  | 313/420 [2:46:28<54:32, 30.58s/it] 75%|███████▍  | 314/420 [2:46:59<54:07, 30.64s/it]                                                   {'loss': 0.9481, 'learning_rate': 5.208845208845209e-06, 'epoch': 2.97}
 75%|███████▍  | 314/420 [2:46:59<54:07, 30.64s/it] 75%|███████▌  | 315/420 [2:47:29<53:19, 30.47s/it]                                                   {'loss': 0.8506, 'learning_rate': 5.1597051597051605e-06, 'epoch': 2.98}
 75%|███████▌  | 315/420 [2:47:29<53:19, 30.47s/it] 75%|███████▌  | 316/420 [2:47:59<52:44, 30.43s/it]                                                   {'loss': 0.8977, 'learning_rate': 5.110565110565111e-06, 'epoch': 2.99}
 75%|███████▌  | 316/420 [2:47:59<52:44, 30.43s/it] 75%|███████▌  | 317/420 [2:48:31<52:35, 30.64s/it]                                                   {'loss': 0.8691, 'learning_rate': 5.061425061425061e-06, 'epoch': 3.0}
 75%|███████▌  | 317/420 [2:48:31<52:35, 30.64s/it][INFO|trainer.py:2889] 2024-07-01 04:16:52,603 >> Saving model checkpoint to ../out/Llama-2-7b-p0.05-lora-seed3-bsz128/tmp-checkpoint-317
[INFO|tokenization_utils_base.py:2432] 2024-07-01 04:16:54,494 >> tokenizer config file saved in ../out/Llama-2-7b-p0.05-lora-seed3-bsz128/tmp-checkpoint-317/tokenizer_config.json
[INFO|tokenization_utils_base.py:2441] 2024-07-01 04:16:54,495 >> Special tokens file saved in ../out/Llama-2-7b-p0.05-lora-seed3-bsz128/tmp-checkpoint-317/special_tokens_map.json
 76%|███████▌  | 318/420 [2:50:08<1:26:03, 50.63s/it]                                                     {'loss': 0.9205, 'learning_rate': 5.012285012285013e-06, 'epoch': 3.01}
 76%|███████▌  | 318/420 [2:50:08<1:26:03, 50.63s/it] 76%|███████▌  | 319/420 [2:50:39<1:15:34, 44.89s/it]                                                     {'loss': 0.9455, 'learning_rate': 4.9631449631449635e-06, 'epoch': 3.02}
 76%|███████▌  | 319/420 [2:50:39<1:15:34, 44.89s/it] 76%|███████▌  | 320/420 [2:51:10<1:07:28, 40.48s/it]                                                     {'loss': 0.781, 'learning_rate': 4.914004914004914e-06, 'epoch': 3.03}
 76%|███████▌  | 320/420 [2:51:10<1:07:28, 40.48s/it] 76%|███████▋  | 321/420 [2:51:41<1:02:13, 37.71s/it]                                                     {'loss': 0.7862, 'learning_rate': 4.864864864864866e-06, 'epoch': 3.04}
 76%|███████▋  | 321/420 [2:51:41<1:02:13, 37.71s/it] 77%|███████▋  | 322/420 [2:52:11<58:08, 35.60s/it]                                                     {'loss': 1.3199, 'learning_rate': 4.815724815724816e-06, 'epoch': 3.05}
 77%|███████▋  | 322/420 [2:52:11<58:08, 35.60s/it] 77%|███████▋  | 323/420 [2:52:42<55:18, 34.22s/it]                                                   {'loss': 0.8487, 'learning_rate': 4.766584766584767e-06, 'epoch': 3.06}
 77%|███████▋  | 323/420 [2:52:42<55:18, 34.22s/it] 77%|███████▋  | 324/420 [2:53:14<53:16, 33.30s/it]                                                   {'loss': 1.4038, 'learning_rate': 4.717444717444718e-06, 'epoch': 3.06}
 77%|███████▋  | 324/420 [2:53:14<53:16, 33.30s/it] 77%|███████▋  | 325/420 [2:53:44<51:18, 32.41s/it]                                                   {'loss': 0.9473, 'learning_rate': 4.668304668304669e-06, 'epoch': 3.07}
 77%|███████▋  | 325/420 [2:53:44<51:18, 32.41s/it] 78%|███████▊  | 326/420 [2:54:15<50:20, 32.13s/it]                                                   {'loss': 0.8456, 'learning_rate': 4.619164619164619e-06, 'epoch': 3.08}
 78%|███████▊  | 326/420 [2:54:15<50:20, 32.13s/it] 78%|███████▊  | 327/420 [2:54:47<49:25, 31.88s/it]                                                   {'loss': 0.9293, 'learning_rate': 4.57002457002457e-06, 'epoch': 3.09}
 78%|███████▊  | 327/420 [2:54:47<49:25, 31.88s/it] 78%|███████▊  | 328/420 [2:55:16<47:47, 31.17s/it]                                                   {'loss': 0.8778, 'learning_rate': 4.520884520884522e-06, 'epoch': 3.1}
 78%|███████▊  | 328/420 [2:55:16<47:47, 31.17s/it] 78%|███████▊  | 329/420 [2:55:46<46:47, 30.85s/it]                                                   {'loss': 0.7512, 'learning_rate': 4.471744471744472e-06, 'epoch': 3.11}
 78%|███████▊  | 329/420 [2:55:46<46:47, 30.85s/it] 79%|███████▊  | 330/420 [2:56:18<46:45, 31.17s/it]                                                   {'loss': 0.7571, 'learning_rate': 4.422604422604423e-06, 'epoch': 3.12}
 79%|███████▊  | 330/420 [2:56:18<46:45, 31.17s/it] 79%|███████▉  | 331/420 [2:56:49<45:56, 30.97s/it]                                                   {'loss': 0.8599, 'learning_rate': 4.373464373464374e-06, 'epoch': 3.13}
 79%|███████▉  | 331/420 [2:56:49<45:56, 30.97s/it] 79%|███████▉  | 332/420 [2:57:20<45:37, 31.10s/it]                                                   {'loss': 0.7923, 'learning_rate': 4.324324324324325e-06, 'epoch': 3.14}
 79%|███████▉  | 332/420 [2:57:20<45:37, 31.10s/it] 79%|███████▉  | 333/420 [2:57:51<45:03, 31.08s/it]                                                   {'loss': 0.8488, 'learning_rate': 4.275184275184275e-06, 'epoch': 3.15}
 79%|███████▉  | 333/420 [2:57:51<45:03, 31.08s/it] 80%|███████▉  | 334/420 [2:58:22<44:23, 30.97s/it]                                                   {'loss': 0.8096, 'learning_rate': 4.226044226044226e-06, 'epoch': 3.16}
 80%|███████▉  | 334/420 [2:58:22<44:23, 30.97s/it] 80%|███████▉  | 335/420 [2:58:53<44:03, 31.10s/it]                                                   {'loss': 0.815, 'learning_rate': 4.176904176904178e-06, 'epoch': 3.17}
 80%|███████▉  | 335/420 [2:58:53<44:03, 31.10s/it] 80%|████████  | 336/420 [2:59:24<43:14, 30.89s/it]                                                   {'loss': 0.8453, 'learning_rate': 4.127764127764128e-06, 'epoch': 3.18}
 80%|████████  | 336/420 [2:59:24<43:14, 30.89s/it] 80%|████████  | 337/420 [2:59:54<42:35, 30.79s/it]                                                   {'loss': 0.876, 'learning_rate': 4.078624078624079e-06, 'epoch': 3.19}
 80%|████████  | 337/420 [2:59:54<42:35, 30.79s/it] 80%|████████  | 338/420 [3:00:25<42:13, 30.89s/it]                                                   {'loss': 0.7898, 'learning_rate': 4.02948402948403e-06, 'epoch': 3.2}
 80%|████████  | 338/420 [3:00:25<42:13, 30.89s/it] 81%|████████  | 339/420 [3:00:58<42:17, 31.33s/it]                                                   {'loss': 1.4683, 'learning_rate': 3.980343980343981e-06, 'epoch': 3.21}
 81%|████████  | 339/420 [3:00:58<42:17, 31.33s/it] 81%|████████  | 340/420 [3:01:28<41:30, 31.13s/it]                                                   {'loss': 1.3404, 'learning_rate': 3.931203931203931e-06, 'epoch': 3.22}
 81%|████████  | 340/420 [3:01:28<41:30, 31.13s/it] 81%|████████  | 341/420 [3:02:00<41:00, 31.15s/it]                                                   {'loss': 0.7911, 'learning_rate': 3.882063882063882e-06, 'epoch': 3.23}
 81%|████████  | 341/420 [3:02:00<41:00, 31.15s/it] 81%|████████▏ | 342/420 [3:02:30<40:03, 30.82s/it]                                                   {'loss': 0.8564, 'learning_rate': 3.832923832923834e-06, 'epoch': 3.23}
 81%|████████▏ | 342/420 [3:02:30<40:03, 30.82s/it] 82%|████████▏ | 343/420 [3:03:01<39:40, 30.91s/it]                                                   {'loss': 0.7532, 'learning_rate': 3.7837837837837844e-06, 'epoch': 3.24}
 82%|████████▏ | 343/420 [3:03:01<39:40, 30.91s/it] 82%|████████▏ | 344/420 [3:03:32<39:23, 31.10s/it]                                                   {'loss': 1.561, 'learning_rate': 3.7346437346437347e-06, 'epoch': 3.25}
 82%|████████▏ | 344/420 [3:03:32<39:23, 31.10s/it] 82%|████████▏ | 345/420 [3:04:02<38:29, 30.79s/it]                                                   {'loss': 0.7849, 'learning_rate': 3.685503685503686e-06, 'epoch': 3.26}
 82%|████████▏ | 345/420 [3:04:02<38:29, 30.79s/it] 82%|████████▏ | 346/420 [3:04:34<38:23, 31.13s/it]                                                   {'loss': 1.3502, 'learning_rate': 3.6363636363636366e-06, 'epoch': 3.27}
 82%|████████▏ | 346/420 [3:04:34<38:23, 31.13s/it] 83%|████████▎ | 347/420 [3:05:05<37:44, 31.02s/it]                                                   {'loss': 0.8566, 'learning_rate': 3.5872235872235877e-06, 'epoch': 3.28}
 83%|████████▎ | 347/420 [3:05:05<37:44, 31.02s/it] 83%|████████▎ | 348/420 [3:05:36<37:05, 30.91s/it]                                                   {'loss': 0.9242, 'learning_rate': 3.538083538083538e-06, 'epoch': 3.29}
 83%|████████▎ | 348/420 [3:05:36<37:05, 30.91s/it] 83%|████████▎ | 349/420 [3:06:06<36:31, 30.87s/it]                                                   {'loss': 1.5027, 'learning_rate': 3.488943488943489e-06, 'epoch': 3.3}
 83%|████████▎ | 349/420 [3:06:06<36:31, 30.87s/it] 83%|████████▎ | 350/420 [3:06:37<35:55, 30.80s/it]                                                   {'loss': 0.7946, 'learning_rate': 3.4398034398034404e-06, 'epoch': 3.31}
 83%|████████▎ | 350/420 [3:06:37<35:55, 30.80s/it] 84%|████████▎ | 351/420 [3:07:08<35:35, 30.95s/it]                                                   {'loss': 0.8802, 'learning_rate': 3.3906633906633907e-06, 'epoch': 3.32}
 84%|████████▎ | 351/420 [3:07:08<35:35, 30.95s/it] 84%|████████▍ | 352/420 [3:07:39<34:57, 30.85s/it]                                                   {'loss': 0.8959, 'learning_rate': 3.341523341523342e-06, 'epoch': 3.33}
 84%|████████▍ | 352/420 [3:07:39<34:57, 30.85s/it] 84%|████████▍ | 353/420 [3:08:09<34:18, 30.72s/it]                                                   {'loss': 0.8042, 'learning_rate': 3.2923832923832925e-06, 'epoch': 3.34}
 84%|████████▍ | 353/420 [3:08:09<34:18, 30.72s/it] 84%|████████▍ | 354/420 [3:08:41<33:54, 30.82s/it]                                                   {'loss': 2.1059, 'learning_rate': 3.2432432432432437e-06, 'epoch': 3.35}
 84%|████████▍ | 354/420 [3:08:41<33:54, 30.82s/it] 85%|████████▍ | 355/420 [3:09:11<33:08, 30.59s/it]                                                   {'loss': 0.8153, 'learning_rate': 3.194103194103194e-06, 'epoch': 3.36}
 85%|████████▍ | 355/420 [3:09:11<33:08, 30.59s/it] 85%|████████▍ | 356/420 [3:09:41<32:37, 30.59s/it]                                                   {'loss': 0.8701, 'learning_rate': 3.144963144963145e-06, 'epoch': 3.37}
 85%|████████▍ | 356/420 [3:09:41<32:37, 30.59s/it] 85%|████████▌ | 357/420 [3:10:12<32:11, 30.65s/it]                                                   {'loss': 0.7601, 'learning_rate': 3.0958230958230963e-06, 'epoch': 3.38}
 85%|████████▌ | 357/420 [3:10:12<32:11, 30.65s/it] 85%|████████▌ | 358/420 [3:10:43<31:50, 30.81s/it]                                                   {'loss': 0.9572, 'learning_rate': 3.046683046683047e-06, 'epoch': 3.39}
 85%|████████▌ | 358/420 [3:10:43<31:50, 30.81s/it] 85%|████████▌ | 359/420 [3:11:13<31:10, 30.66s/it]                                                   {'loss': 0.9498, 'learning_rate': 2.9975429975429978e-06, 'epoch': 3.4}
 85%|████████▌ | 359/420 [3:11:13<31:10, 30.66s/it] 86%|████████▌ | 360/420 [3:11:45<30:49, 30.82s/it]                                                   {'loss': 0.9446, 'learning_rate': 2.9484029484029485e-06, 'epoch': 3.41}
 86%|████████▌ | 360/420 [3:11:45<30:49, 30.82s/it] 86%|████████▌ | 361/420 [3:12:16<30:22, 30.88s/it]                                                   {'loss': 0.8126, 'learning_rate': 2.8992628992628997e-06, 'epoch': 3.41}
 86%|████████▌ | 361/420 [3:12:16<30:22, 30.88s/it] 86%|████████▌ | 362/420 [3:12:46<29:39, 30.68s/it]                                                   {'loss': 0.8669, 'learning_rate': 2.850122850122851e-06, 'epoch': 3.42}
 86%|████████▌ | 362/420 [3:12:46<29:39, 30.68s/it] 86%|████████▋ | 363/420 [3:13:17<29:23, 30.94s/it]                                                   {'loss': 0.8754, 'learning_rate': 2.800982800982801e-06, 'epoch': 3.43}
 86%|████████▋ | 363/420 [3:13:17<29:23, 30.94s/it] 87%|████████▋ | 364/420 [3:13:47<28:34, 30.62s/it]                                                   {'loss': 0.8918, 'learning_rate': 2.751842751842752e-06, 'epoch': 3.44}
 87%|████████▋ | 364/420 [3:13:47<28:34, 30.62s/it] 87%|████████▋ | 365/420 [3:14:18<28:07, 30.69s/it]                                                   {'loss': 0.9463, 'learning_rate': 2.702702702702703e-06, 'epoch': 3.45}
 87%|████████▋ | 365/420 [3:14:18<28:07, 30.69s/it] 87%|████████▋ | 366/420 [3:14:48<27:30, 30.56s/it]                                                   {'loss': 0.8514, 'learning_rate': 2.6535626535626533e-06, 'epoch': 3.46}
 87%|████████▋ | 366/420 [3:14:48<27:30, 30.56s/it] 87%|████████▋ | 367/420 [3:15:19<27:01, 30.60s/it]                                                   {'loss': 0.8321, 'learning_rate': 2.6044226044226045e-06, 'epoch': 3.47}
 87%|████████▋ | 367/420 [3:15:19<27:01, 30.60s/it] 88%|████████▊ | 368/420 [3:15:50<26:35, 30.68s/it]                                                   {'loss': 0.8688, 'learning_rate': 2.5552825552825556e-06, 'epoch': 3.48}
 88%|████████▊ | 368/420 [3:15:50<26:35, 30.68s/it] 88%|████████▊ | 369/420 [3:16:21<26:11, 30.80s/it]                                                   {'loss': 0.8325, 'learning_rate': 2.5061425061425064e-06, 'epoch': 3.49}
 88%|████████▊ | 369/420 [3:16:21<26:11, 30.80s/it] 88%|████████▊ | 370/420 [3:16:52<25:43, 30.88s/it]                                                   {'loss': 1.4874, 'learning_rate': 2.457002457002457e-06, 'epoch': 3.5}
 88%|████████▊ | 370/420 [3:16:52<25:43, 30.88s/it] 88%|████████▊ | 371/420 [3:17:23<25:14, 30.90s/it]                                                   {'loss': 0.8964, 'learning_rate': 2.407862407862408e-06, 'epoch': 3.51}
 88%|████████▊ | 371/420 [3:17:23<25:14, 30.90s/it] 89%|████████▊ | 372/420 [3:17:54<24:47, 30.99s/it]                                                   {'loss': 0.8079, 'learning_rate': 2.358722358722359e-06, 'epoch': 3.52}
 89%|████████▊ | 372/420 [3:17:54<24:47, 30.99s/it] 89%|████████▉ | 373/420 [3:18:25<24:19, 31.06s/it]                                                   {'loss': 0.9478, 'learning_rate': 2.3095823095823097e-06, 'epoch': 3.53}
 89%|████████▉ | 373/420 [3:18:25<24:19, 31.06s/it] 89%|████████▉ | 374/420 [3:18:57<23:51, 31.12s/it]                                                   {'loss': 0.8637, 'learning_rate': 2.260442260442261e-06, 'epoch': 3.54}
 89%|████████▉ | 374/420 [3:18:57<23:51, 31.12s/it] 89%|████████▉ | 375/420 [3:19:27<23:10, 30.90s/it]                                                   {'loss': 0.8315, 'learning_rate': 2.2113022113022116e-06, 'epoch': 3.55}
 89%|████████▉ | 375/420 [3:19:27<23:10, 30.90s/it] 90%|████████▉ | 376/420 [3:19:57<22:31, 30.71s/it]                                                   {'loss': 0.796, 'learning_rate': 2.1621621621621623e-06, 'epoch': 3.56}
 90%|████████▉ | 376/420 [3:19:57<22:31, 30.71s/it] 90%|████████▉ | 377/420 [3:20:29<22:16, 31.07s/it]                                                   {'loss': 1.1642, 'learning_rate': 2.113022113022113e-06, 'epoch': 3.57}
 90%|████████▉ | 377/420 [3:20:29<22:16, 31.07s/it] 90%|█████████ | 378/420 [3:21:00<21:42, 31.01s/it]                                                   {'loss': 0.9529, 'learning_rate': 2.063882063882064e-06, 'epoch': 3.58}
 90%|█████████ | 378/420 [3:21:00<21:42, 31.01s/it] 90%|█████████ | 379/420 [3:21:31<21:09, 30.97s/it]                                                   {'loss': 2.2668, 'learning_rate': 2.014742014742015e-06, 'epoch': 3.58}
 90%|█████████ | 379/420 [3:21:31<21:09, 30.97s/it] 90%|█████████ | 380/420 [3:22:02<20:41, 31.04s/it]                                                   {'loss': 0.8124, 'learning_rate': 1.9656019656019657e-06, 'epoch': 3.59}
 90%|█████████ | 380/420 [3:22:02<20:41, 31.04s/it] 91%|█████████ | 381/420 [3:22:34<20:15, 31.16s/it]                                                   {'loss': 0.9009, 'learning_rate': 1.916461916461917e-06, 'epoch': 3.6}
 91%|█████████ | 381/420 [3:22:34<20:15, 31.16s/it] 91%|█████████ | 382/420 [3:23:05<19:46, 31.24s/it]                                                   {'loss': 0.7547, 'learning_rate': 1.8673218673218673e-06, 'epoch': 3.61}
 91%|█████████ | 382/420 [3:23:05<19:46, 31.24s/it] 91%|█████████ | 383/420 [3:23:37<19:20, 31.36s/it]                                                   {'loss': 0.8035, 'learning_rate': 1.8181818181818183e-06, 'epoch': 3.62}
 91%|█████████ | 383/420 [3:23:37<19:20, 31.36s/it] 91%|█████████▏| 384/420 [3:24:09<18:58, 31.63s/it]                                                   {'loss': 0.7866, 'learning_rate': 1.769041769041769e-06, 'epoch': 3.63}
 91%|█████████▏| 384/420 [3:24:09<18:58, 31.63s/it] 92%|█████████▏| 385/420 [3:24:39<18:05, 31.00s/it]                                                   {'loss': 0.8943, 'learning_rate': 1.7199017199017202e-06, 'epoch': 3.64}
 92%|█████████▏| 385/420 [3:24:39<18:05, 31.00s/it] 92%|█████████▏| 386/420 [3:25:09<17:30, 30.88s/it]                                                   {'loss': 0.8897, 'learning_rate': 1.670761670761671e-06, 'epoch': 3.65}
 92%|█████████▏| 386/420 [3:25:09<17:30, 30.88s/it] 92%|█████████▏| 387/420 [3:25:40<16:58, 30.88s/it]                                                   {'loss': 1.7844, 'learning_rate': 1.6216216216216219e-06, 'epoch': 3.66}
 92%|█████████▏| 387/420 [3:25:40<16:58, 30.88s/it] 92%|█████████▏| 388/420 [3:26:11<16:24, 30.76s/it]                                                   {'loss': 0.8222, 'learning_rate': 1.5724815724815726e-06, 'epoch': 3.67}
 92%|█████████▏| 388/420 [3:26:11<16:24, 30.76s/it] 93%|█████████▎| 389/420 [3:26:41<15:54, 30.79s/it]                                                   {'loss': 1.456, 'learning_rate': 1.5233415233415235e-06, 'epoch': 3.68}
 93%|█████████▎| 389/420 [3:26:41<15:54, 30.79s/it] 93%|█████████▎| 390/420 [3:27:13<15:28, 30.94s/it]                                                   {'loss': 1.2093, 'learning_rate': 1.4742014742014743e-06, 'epoch': 3.69}
 93%|█████████▎| 390/420 [3:27:13<15:28, 30.94s/it] 93%|█████████▎| 391/420 [3:27:44<15:00, 31.06s/it]                                                   {'loss': 0.9081, 'learning_rate': 1.4250614250614254e-06, 'epoch': 3.7}
 93%|█████████▎| 391/420 [3:27:44<15:00, 31.06s/it] 93%|█████████▎| 392/420 [3:28:15<14:26, 30.96s/it]                                                   {'loss': 1.3676, 'learning_rate': 1.375921375921376e-06, 'epoch': 3.71}
 93%|█████████▎| 392/420 [3:28:15<14:26, 30.96s/it] 94%|█████████▎| 393/420 [3:28:45<13:52, 30.85s/it]                                                   {'loss': 0.8677, 'learning_rate': 1.3267813267813267e-06, 'epoch': 3.72}
 94%|█████████▎| 393/420 [3:28:45<13:52, 30.85s/it] 94%|█████████▍| 394/420 [3:29:16<13:19, 30.75s/it]                                                   {'loss': 0.8431, 'learning_rate': 1.2776412776412778e-06, 'epoch': 3.73}
 94%|█████████▍| 394/420 [3:29:16<13:19, 30.75s/it] 94%|█████████▍| 395/420 [3:29:46<12:46, 30.64s/it]                                                   {'loss': 0.8693, 'learning_rate': 1.2285012285012285e-06, 'epoch': 3.74}
 94%|█████████▍| 395/420 [3:29:46<12:46, 30.64s/it] 94%|█████████▍| 396/420 [3:30:17<12:13, 30.55s/it]                                                   {'loss': 0.7546, 'learning_rate': 1.1793611793611795e-06, 'epoch': 3.75}
 94%|█████████▍| 396/420 [3:30:17<12:13, 30.55s/it] 95%|█████████▍| 397/420 [3:30:47<11:43, 30.59s/it]                                                   {'loss': 0.9335, 'learning_rate': 1.1302211302211304e-06, 'epoch': 3.75}
 95%|█████████▍| 397/420 [3:30:47<11:43, 30.59s/it] 95%|█████████▍| 398/420 [3:31:18<11:12, 30.55s/it]                                                   {'loss': 0.9109, 'learning_rate': 1.0810810810810812e-06, 'epoch': 3.76}
 95%|█████████▍| 398/420 [3:31:18<11:12, 30.55s/it] 95%|█████████▌| 399/420 [3:31:49<10:43, 30.65s/it]                                                   {'loss': 0.7976, 'learning_rate': 1.031941031941032e-06, 'epoch': 3.77}
 95%|█████████▌| 399/420 [3:31:49<10:43, 30.65s/it] 95%|█████████▌| 400/420 [3:32:19<10:14, 30.73s/it]                                                   {'loss': 0.819, 'learning_rate': 9.828009828009828e-07, 'epoch': 3.78}
 95%|█████████▌| 400/420 [3:32:19<10:14, 30.73s/it] 95%|█████████▌| 401/420 [3:32:50<09:40, 30.53s/it]                                                   {'loss': 1.6268, 'learning_rate': 9.336609336609337e-07, 'epoch': 3.79}
 95%|█████████▌| 401/420 [3:32:50<09:40, 30.53s/it] 96%|█████████▌| 402/420 [3:33:21<09:12, 30.67s/it]                                                   {'loss': 0.9221, 'learning_rate': 8.845208845208845e-07, 'epoch': 3.8}
 96%|█████████▌| 402/420 [3:33:21<09:12, 30.67s/it] 96%|█████████▌| 403/420 [3:33:51<08:41, 30.65s/it]                                                   {'loss': 0.8719, 'learning_rate': 8.353808353808355e-07, 'epoch': 3.81}
 96%|█████████▌| 403/420 [3:33:51<08:41, 30.65s/it] 96%|█████████▌| 404/420 [3:34:22<08:12, 30.79s/it]                                                   {'loss': 0.8485, 'learning_rate': 7.862407862407863e-07, 'epoch': 3.82}
 96%|█████████▌| 404/420 [3:34:22<08:12, 30.79s/it] 96%|█████████▋| 405/420 [3:34:53<07:43, 30.88s/it]                                                   {'loss': 0.8599, 'learning_rate': 7.371007371007371e-07, 'epoch': 3.83}
 96%|█████████▋| 405/420 [3:34:53<07:43, 30.88s/it] 97%|█████████▋| 406/420 [3:35:23<07:07, 30.50s/it]                                                   {'loss': 0.9192, 'learning_rate': 6.87960687960688e-07, 'epoch': 3.84}
 97%|█████████▋| 406/420 [3:35:23<07:07, 30.50s/it] 97%|█████████▋| 407/420 [3:35:55<06:40, 30.85s/it]                                                   {'loss': 0.8396, 'learning_rate': 6.388206388206389e-07, 'epoch': 3.85}
 97%|█████████▋| 407/420 [3:35:55<06:40, 30.85s/it] 97%|█████████▋| 408/420 [3:36:25<06:09, 30.78s/it]                                                   {'loss': 0.768, 'learning_rate': 5.896805896805897e-07, 'epoch': 3.86}
 97%|█████████▋| 408/420 [3:36:25<06:09, 30.78s/it] 97%|█████████▋| 409/420 [3:36:57<05:42, 31.09s/it]                                                   {'loss': 1.658, 'learning_rate': 5.405405405405406e-07, 'epoch': 3.87}
 97%|█████████▋| 409/420 [3:36:57<05:42, 31.09s/it] 98%|█████████▊| 410/420 [3:37:28<05:09, 30.95s/it]                                                   {'loss': 1.6849, 'learning_rate': 4.914004914004914e-07, 'epoch': 3.88}
 98%|█████████▊| 410/420 [3:37:28<05:09, 30.95s/it] 98%|█████████▊| 411/420 [3:37:58<04:36, 30.73s/it]                                                   {'loss': 3.5182, 'learning_rate': 4.4226044226044226e-07, 'epoch': 3.89}
 98%|█████████▊| 411/420 [3:37:58<04:36, 30.73s/it] 98%|█████████▊| 412/420 [3:38:29<04:06, 30.77s/it]                                                   {'loss': 1.0839, 'learning_rate': 3.9312039312039315e-07, 'epoch': 3.9}
 98%|█████████▊| 412/420 [3:38:29<04:06, 30.77s/it] 98%|█████████▊| 413/420 [3:38:59<03:33, 30.57s/it]                                                   {'loss': 0.9005, 'learning_rate': 3.43980343980344e-07, 'epoch': 3.91}
 98%|█████████▊| 413/420 [3:38:59<03:33, 30.57s/it] 99%|█████████▊| 414/420 [3:39:30<03:05, 30.85s/it]                                                   {'loss': 0.8734, 'learning_rate': 2.9484029484029487e-07, 'epoch': 3.92}
 99%|█████████▊| 414/420 [3:39:30<03:05, 30.85s/it] 99%|█████████▉| 415/420 [3:40:03<02:36, 31.27s/it]                                                   {'loss': 1.0212, 'learning_rate': 2.457002457002457e-07, 'epoch': 3.93}
 99%|█████████▉| 415/420 [3:40:03<02:36, 31.27s/it] 99%|█████████▉| 416/420 [3:40:32<02:03, 30.80s/it]                                                   {'loss': 0.8609, 'learning_rate': 1.9656019656019657e-07, 'epoch': 3.93}
 99%|█████████▉| 416/420 [3:40:32<02:03, 30.80s/it] 99%|█████████▉| 417/420 [3:41:03<01:32, 30.71s/it]                                                   {'loss': 0.9143, 'learning_rate': 1.4742014742014744e-07, 'epoch': 3.94}
 99%|█████████▉| 417/420 [3:41:03<01:32, 30.71s/it]100%|█████████▉| 418/420 [3:41:34<01:01, 30.77s/it]                                                   {'loss': 0.8171, 'learning_rate': 9.828009828009829e-08, 'epoch': 3.95}
100%|█████████▉| 418/420 [3:41:34<01:01, 30.77s/it]100%|█████████▉| 419/420 [3:42:05<00:30, 30.88s/it]                                                   {'loss': 0.8834, 'learning_rate': 4.914004914004914e-08, 'epoch': 3.96}
100%|█████████▉| 419/420 [3:42:05<00:30, 30.88s/it]100%|██████████| 420/420 [3:42:37<00:00, 31.24s/it]                                                   {'loss': 0.906, 'learning_rate': 0.0, 'epoch': 3.97}
100%|██████████| 420/420 [3:42:37<00:00, 31.24s/it][INFO|trainer.py:2889] 2024-07-01 05:10:53,135 >> Saving model checkpoint to ../out/Llama-2-7b-p0.05-lora-seed3-bsz128/tmp-checkpoint-420
[INFO|tokenization_utils_base.py:2432] 2024-07-01 05:10:54,992 >> tokenizer config file saved in ../out/Llama-2-7b-p0.05-lora-seed3-bsz128/tmp-checkpoint-420/tokenizer_config.json
[INFO|tokenization_utils_base.py:2441] 2024-07-01 05:10:54,993 >> Special tokens file saved in ../out/Llama-2-7b-p0.05-lora-seed3-bsz128/tmp-checkpoint-420/special_tokens_map.json
[INFO|trainer.py:1947] 2024-07-01 05:12:02,943 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


                                                   {'train_runtime': 13446.6372, 'train_samples_per_second': 4.026, 'train_steps_per_second': 0.031, 'train_loss': 1.1637763734374726, 'epoch': 3.97}
100%|██████████| 420/420 [3:43:47<00:00, 31.24s/it]100%|██████████| 420/420 [3:43:47<00:00, 31.97s/it]
[INFO|trainer.py:2889] 2024-07-01 05:12:02,990 >> Saving model checkpoint to ../out/Llama-2-7b-p0.05-lora-seed3-bsz128
[INFO|tokenization_utils_base.py:2432] 2024-07-01 05:12:04,575 >> tokenizer config file saved in ../out/Llama-2-7b-p0.05-lora-seed3-bsz128/tokenizer_config.json
[INFO|tokenization_utils_base.py:2441] 2024-07-01 05:12:04,575 >> Special tokens file saved in ../out/Llama-2-7b-p0.05-lora-seed3-bsz128/special_tokens_map.json
***** train metrics *****
  epoch                    =       3.97
  train_loss               =     1.1638
  train_runtime            = 3:44:06.63
  train_samples            =      13533
  train_samples_per_second =      4.026
  train_steps_per_second   =      0.031
wandb: - 0.021 MB of 0.021 MB uploadedwandb: \ 0.021 MB of 0.068 MB uploadedwandb: 
wandb: Run history:
wandb:                    train/epoch ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███
wandb:              train/global_step ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███
wandb:            train/learning_rate ▃████▇▇▇▇▇▇▆▆▆▆▆▅▅▅▅▅▄▄▄▄▄▃▃▃▃▃▃▂▂▂▂▂▁▁▁
wandb:                     train/loss █▄▂▄▂▂▂▂▃▁▁▂▃▁▁▁▁▃▁▂▁▃▁▁▁▁▁▁▁▂▃▁▃▁▁▁▁▁▁▁
wandb:               train/total_flos ▁
wandb:               train/train_loss ▁
wandb:            train/train_runtime ▁
wandb: train/train_samples_per_second ▁
wandb:   train/train_steps_per_second ▁
wandb: 
wandb: Run summary:
wandb:                    train/epoch 3.97
wandb:              train/global_step 420
wandb:            train/learning_rate 0.0
wandb:                     train/loss 0.906
wandb:               train/total_flos 8.065941790806508e+17
wandb:               train/train_loss 1.16378
wandb:            train/train_runtime 13446.6372
wandb: train/train_samples_per_second 4.026
wandb:   train/train_steps_per_second 0.031
wandb: 
wandb: 🚀 View run silvery-bush-1 at: https://wandb.ai/raidriar_dai/LESS_Reproduce/runs/q8yi57rg
wandb: ⭐️ View project at: https://wandb.ai/raidriar_dai/LESS_Reproduce
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240701_012800-q8yi57rg/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
